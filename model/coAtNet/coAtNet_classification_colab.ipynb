{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28360,"status":"ok","timestamp":1691939118681,"user":{"displayName":"재원E import","userId":"05798677231600828314"},"user_tz":-540},"id":"d9dnHQxLbYgR","outputId":"7fdbb0f9-5e54-45b5-d8e5-0cd69959969f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":5973,"status":"ok","timestamp":1691939085881,"user":{"displayName":"재원E import","userId":"05798677231600828314"},"user_tz":-540},"id":"5jltPzsEVHhe"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision.transforms import Compose, ToTensor\n","from torch.utils.data import DataLoader\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","from scipy.io import wavfile\n","import matplotlib.pyplot as plt\n","import librosa\n","import os\n","import sys\n"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1691939392737,"user":{"displayName":"재원E import","userId":"05798677231600828314"},"user_tz":-540},"id":"EX8HWMsLVHhi"},"outputs":[],"source":["# main_path = '/content/drive/MyDrive/baby_cry'\n","# data_path = os.path.join(main_path, 'data2')\n","\n","# AUDIO_DIR = os.path.join(main_path, 'audios')\n","# MODEL_PATH = os.path.join(main_path, 'coAtNet.pt')"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1691939166101,"user":{"displayName":"재원E import","userId":"05798677231600828314"},"user_tz":-540},"id":"kVsnv9cPVHhi"},"outputs":[],"source":["main_path = '/Users/jaewone/developer/tensorflow/baby-cry-classification'\n","data_path = os.path.join(main_path, 'data')\n","\n","work_path = os.path.join(main_path, 'model', 'coAtNet')\n","AUDIO_DIR = os.path.join(work_path, 'audios2')\n","MODEL_PATH = os.path.join(work_path, 'model.pt')\n","\n","sys.path.append(main_path)"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1691939498693,"user":{"displayName":"재원E import","userId":"05798677231600828314"},"user_tz":-540},"id":"g87bjdCNVHhj"},"outputs":[],"source":["# # copy file\n","# from shutil import copyfile\n","# from typing import Union, Optional\n","# from random import sample, seed, choices, shuffle\n","# from math import isclose\n","\n","# def copy_file(src: str, dst: str):\n","#     if os.path.isfile(src):\n","#         copyfile(src, dst)\n","#     else:\n","#         raise OSError(f'{src} is not a file')\n","\n","\n","# # 각각의 state에서 n개의 무작위 파일을 선택하여 경로를 반환한다.\n","# def get_state_samples(data_path: str,\n","#                       state_list: Optional[list[str]] = None,\n","#                       n_extract: int = 100,\n","#                       rand_seed: int = 123) -> list[str]:\n","#     \"\"\"\n","#     각각의 state에서 n개의 무작위 파일을 선택하여 경로를 반환한다.\n","\n","#     Parameters:\n","\n","#         * data_path : 파일의 경로\n","\n","#         * state_list=None : state 리스트를 받을 경우 state_list가 포함하는 state 폴더의 파일들만 이름을 변경한다.\n","\n","#         * n_extract=100 : 각각의 state 별 추출할 파일 개수. 만약 n_etract 보다 파일 개수가 부족하다면 파일의 개수 만큼만 추출한다.\n","\n","#         * rand_seed=123 : 난수를 생성하는 시드값으로 동일한 시드값은 동일한 결과를 보장한다.\n","\n","#     Returns: 랜덤 추출한 파일의 경로 리스트\n","#     \"\"\"\n","\n","#     if not os.path.exists(data_path):\n","#         raise OSError(f'path {data_path} not exist.')\n","\n","#     state_list = ['sleepy', 'uncomfortable', 'diaper', 'awake', 'sad', 'hug', 'hungry']\n","\n","#     seed(rand_seed)\n","#     file_list = []\n","#     for state in state_list:\n","#         state_file_list = [os.path.join(data_path, state, file) for file in os.listdir(\n","#             os.path.join(data_path, state))]\n","#         file_list = file_list + (sample(state_file_list, k=n_extract)\n","#                                  if len(state_file_list) > n_extract else state_file_list)\n","\n","#     return file_list\n","\n","# # 각각의 state에서 n개의 무작위 파일을 추출한다.\n","# def extract_state_sample(data_path: str,\n","#                          output_dir: str,\n","#                          n_extract: int = 100,\n","#                          rand_seed: int = 123,\n","#                          state_list: Optional[list[str]] = None,\n","#                          with_dir: bool = True) -> list[str]:\n","#     \"\"\"\n","#     각각의 state에서 n개의 무작위 파일을 추출한다.\n","\n","#     Parameters:\n","\n","#         * data_path : 파일의 경로\n","\n","#         * output_dir : sample 파일들을 저장할 폴더 경로. 만약 경로가 이미 존재 할 경우 예외를 발생시키며 경로가 없으면 생성한다.\n","\n","#         * n_extract=100 : 각각의 state 별 추출할 파일 개수. 만약 n_etract 보다 파일 개수가 부족하다면 파일의 개수 만큼만 추출한다.\n","\n","#         * rand_seed=123 : 난수를 생성하는 시드값으로 동일한 시드값은 동일한 결과를 보장한다.\n","\n","#         * state_list=None : state 리스트를 받을 경우 state_list가 포함하는 state 폴더의 파일들만 이름을 변경한다.\n","\n","#         * with_dir=True : False일 경우 output_path에 state에 따른 폴더 구분 없이 데이터를 추출한다.\n","\n","#     Returns: 랜덤 추출하여 저장된 파일의 경로 리스트\n","#     \"\"\"\n","\n","#     # Check output_dir exist.\n","#     if os.path.exists(output_dir):\n","#         raise OSError(f'path {output_dir} already exist.')\n","#     else:\n","#         os.mkdir(output_dir)\n","\n","#     state_list = ['sleepy', 'uncomfortable', 'diaper', 'awake', 'sad', 'hug', 'hungry']\n","\n","#     # Create folder of state if with_dir is True\n","#     if with_dir:\n","#         for state in state_list:\n","#             os.mkdir(os.path.join(output_dir, state))\n","\n","#     # Extract n sample of state\n","#     sample_data_list = get_state_samples(\n","#         data_path, state_list, n_extract, rand_seed)\n","\n","#     # Copy files with sample data list\n","#     output_file_list = []\n","#     for file in sample_data_list:\n","#         if with_dir:\n","#             _, dir, filename = file.rsplit('/', 2)\n","#             output_file = os.path.join(output_dir, dir, filename)\n","#             output_file_list.append(output_file)\n","#             copy_file(file, output_file)\n","#         else:\n","#             output_file = os.path.join(output_dir, file.rsplit('/', 1)[1])\n","#             output_file_list.append(output_file)\n","#             copy_file(file, output_file)\n","\n","#     return output_file_list\n"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":42641,"status":"ok","timestamp":1691939543738,"user":{"displayName":"재원E import","userId":"05798677231600828314"},"user_tz":-540},"id":"c2J_FvphVHhj"},"outputs":[],"source":["from trans_data import extract_state_sample, get_state_file_list\n","\n","if not os.path.exists(AUDIO_DIR):\n","    extract_state_sample(data_path, AUDIO_DIR, 10, with_dir=True)"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1691939543741,"user":{"displayName":"재원E import","userId":"05798677231600828314"},"user_tz":-540},"id":"zCGQVLvPdD1c","outputId":"061179ec-4f5a-4f8f-863f-83bef9e0413a"},"outputs":[{"data":{"text/plain":["False"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["torch.cuda.is_available()"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":478,"status":"ok","timestamp":1691939573127,"user":{"displayName":"재원E import","userId":"05798677231600828314"},"user_tz":-540},"id":"Qn-nMlfOVHhk"},"outputs":[],"source":["class CoAtNet(nn.Module):\n","    def __init__(self, num_classes=36):\n","        super(CoAtNet, self).__init__()\n","\n","        # Convolutional part\n","        self.conv_layers = nn.Sequential(\n","            # in_channels = 1 : Number of channels in the input image\n","            # out_channels = 32 : Number of channels produced by the convolution\n","            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(32),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","        )\n","\n","        # Transformer part\n","        encoder_layer = nn.TransformerEncoderLayer(d_model=32, nhead=8)\n","        self.transformer_encoder = nn.TransformerEncoder(\n","            encoder_layer, num_layers=2)\n","\n","        # Linear classifier\n","        self.fc = nn.Linear(32, num_classes)\n","\n","    def forward(self, x):\n","        x = self.conv_layers(x)\n","\n","        # Flattening\n","        x = x.view(x.size(0), -1, x.size(1))\n","\n","        # Transformer encoder\n","        x = self.transformer_encoder(x)\n","\n","        # Max pooling over time\n","        x, _ = torch.max(x, dim=1)\n","\n","        # Classifier\n","        x = self.fc(x)\n","        return x\n"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1691939588810,"user":{"displayName":"재원E import","userId":"05798677231600828314"},"user_tz":-540},"id":"8je93eBwVHhk","outputId":"29811556-c7bd-42fe-ea63-1ed81ebda915"},"outputs":[{"name":"stdout","output_type":"stream","text":["/Users/jaewone/developer/tensorflow/baby-cry-classification/model/coAtNet/audios2/sad/sad_326.wav\n"]},{"data":{"text/plain":["70"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["file_list = get_state_file_list(AUDIO_DIR)\n","print(file_list[0])\n","len(file_list)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["class ToMelSpectrogram:\n","    def __call__(self, samples):\n","        # print(samples.shape) # (16000,)\n","        mel_spectrogram = librosa.feature.melspectrogram(y=samples, sr=16000, n_mels=64, hop_length=225)\n","        log_mel_spectrogram = librosa.power_to_db(mel_spectrogram)\n","        return log_mel_spectrogram\n","\n","class AudioDataset(torch.utils.data.Dataset):\n","    def __init__(self, file_list, transform=None):\n","        self.file_list = file_list\n","        self.transform = transform\n","        self.class_map = {'sleepy': 0, 'uncomfortable': 1, 'diaper': 2, 'awake': 3, 'sad': 4, 'hug': 5, 'hungry': 6}\n","        self.label_list = np.array([self.class_mapping(file.rsplit('/', 2)[1]) for file in file_list])\n","\n","    def class_mapping(self, class_name:str) -> int:\n","        return self.class_map[class_name]\n","\n","    def __len__(self):\n","        return len(self.file_list)\n","\n","    def __getitem__(self, idx):\n","        waveform, _ = librosa.load(self.file_list[idx],\n","                                   sr=None,\n","                                   duration=1.0,\n","                                   mono=True)\n","        label = self.label_list[idx]\n","\n","        if self.transform:\n","            waveform = self.transform(waveform)\n","\n","        return waveform, label\n","\n"]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":464,"status":"ok","timestamp":1691940453839,"user":{"displayName":"재원E import","userId":"05798677231600828314"},"user_tz":-540},"id":"V6jt1leBgm50"},"outputs":[],"source":["# class ToMelSpectrogram:\n","#     def __call__(self, samples):\n","#         # print(samples.shape) # (16000,)\n","#         return librosa.feature.melspectrogram(y=samples, sr=16000, n_mels=64, hop_length=225)\n","\n","# class AudioDataset(torch.utils.data.Dataset):\n","#     def __init__(self, data_dir, transform=None):\n","#         self.data_dir = data_dir\n","#         self.transform = transform\n","#         self.file_list = os.listdir(self.data_dir)\n","#         self.class_map = {'sleepy': 0, 'uncomfortable': 1, 'diaper': 2, 'awake': 3, 'sad': 4, 'hug': 5, 'hungry': 6}\n","#         self.label_list = np.array([self.class_mapping(file.split('_')[0]) for file in file_list])\n","\n","#     def class_mapping(self, class_name:str) -> int:\n","#         return self.class_map[class_name]\n","\n","#     def __len__(self):\n","#         return len(self.file_list)\n","\n","#     def __getitem__(self, idx):\n","#         waveform, _ = librosa.load(os.path.join(self.data_dir, self.file_list[idx]),\n","#                                    sr=None,\n","#                                    duration=1.0,\n","#                                    mono=True)\n","#         label = self.label_list[idx]\n","\n","#         if self.transform:\n","#             waveform = self.transform(waveform)\n","\n","#         return waveform, label\n","\n"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["49\n","14\n","7\n"]}],"source":["train_file_list, val_file_list = train_test_split(file_list, test_size=0.3, stratify=[file.rsplit('/', 2)[1] for file in file_list])\n","val_file_list, test_file_list = train_test_split(val_file_list, test_size=0.33, stratify=[file.rsplit('/', 2)[1] for file in val_file_list])\n","\n","print(len(train_file_list))\n","print(len(val_file_list))\n","print(len(test_file_list))\n","\n","\n","with open(os.path.join(work_path, \"test_file_list.txt\"), \"w\") as txt_file:\n","    # txt_file.write(str([file.rsplit('/', 1)[1] for file in test_file_list]))\n","    txt_file.write(str(test_file_list))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12802,"status":"ok","timestamp":1691940468440,"user":{"displayName":"재원E import","userId":"05798677231600828314"},"user_tz":-540},"id":"vHvLwzqidPpx","outputId":"d94a9dc4-065c-47c2-cc1e-2d5c466aa12a"},"outputs":[],"source":["def train():\n","    # We will use the transformation to convert the audio into Mel spectrogram\n","    transform = Compose([ToMelSpectrogram(), ToTensor()])\n","\n","    train_set = AudioDataset(train_file_list, transform=transform)\n","    val_set = AudioDataset(val_file_list, transform=transform)\n","\n","    train_loader = DataLoader(dataset=train_set, batch_size=16, shuffle=True)\n","    val_loader = DataLoader(dataset=val_set, batch_size=16, shuffle=True)\n","\n","    # Assuming we have this class implemented following the paper or using a library\n","    model = CoAtNet()\n","    model = model.cuda()\n","    optimizer = optim.Adam(model.parameters(), lr=5e-4)\n","    criterion = nn.CrossEntropyLoss().cuda()\n","\n","    num_epochs = 10\n","\n","    for epoch in range(num_epochs):\n","        model.train()\n","        for data in train_loader:\n","            inputs, labels = data\n","            inputs, labels = inputs.cuda(), labels.cuda()  # Move both tensors to GPU\n","\n","\n","            optimizer.zero_grad()\n","\n","            # Forward pass\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","\n","            # Backward and optimize\n","            loss.backward()\n","            optimizer.step()\n","\n","        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item()}\")\n","\n","        # Validation\n","        if (epoch + 1) % 5 == 0:\n","            model.eval()\n","            with torch.no_grad():\n","                correct = 0\n","                total = 0\n","                for inputs, labels in val_loader:\n","                    inputs = inputs.cuda()\n","                    labels = labels.cuda()\n","                    outputs = model(inputs)\n","                    _, predicted = torch.max(outputs.data, 1)\n","                    total += labels.size(0)\n","                    correct += (predicted == labels).sum().item()\n","\n","                print(f\"Validation Accuracy: {correct/total}\")\n","\n","    torch.save(model.state_dict(), MODEL_PATH)\n","\n","# train()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["transform = Compose([ToMelSpectrogram(), ToTensor()])\n","test_set = AudioDataset(test_file_list, transform=transform)\n","test_loader = DataLoader(dataset=test_set, batch_size=16, shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class PredictDataset(torch.utils.data.Dataset):\n","    def __init__(self, audio_paths, transform=None):\n","        self.audio_paths = audio_paths\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.audio_paths)\n","\n","    def __getitem__(self, idx):\n","        audio_path = self.audio_paths[idx]\n","        audio_clip = load_audio_clip(audio_path)\n","\n","        if self.transform:\n","            audio_clip = self.transform(audio_clip)\n","\n","        return audio_clip\n","\n","\n","def load_model(path):\n","    model = CoAtNet()  # should match the architecture of the trained model\n","    model.load_state_dict(torch.load(path))\n","    model.eval()\n","    return model\n","\n","\n","def predict(audio_paths):\n","    model = load_model(MODEL_PATH)\n","\n","    transform = transforms.Compose([\n","        # add transforms that were used while training\n","    ])\n","\n","    dataset = PredictDataset(audio_paths, transform=transform)\n","    data_loader = DataLoader(dataset, batch_size=1, shuffle=False)\n","\n","    predictions = []\n","\n","    for batch in data_loader:\n","        batch = batch.cuda()\n","        outputs = model(batch)\n","        # change if multi-label classification\n","        _, predicted = torch.max(outputs.data, 1)\n","        predictions.append(predicted.item())\n","\n","    return predictions\n","\n","\n","def main():\n","    audio_paths = [\"audio1.wav\", \"audio2.wav\",\n","                   \"audio3.wav\"]  # replace with actual paths\n","    predictions = predict(audio_paths)\n","    print(predictions)\n","\n","\n","if __name__ == \"__main__\":\n","    main()\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.0"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
