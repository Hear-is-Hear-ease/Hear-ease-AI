{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37bdf851",
   "metadata": {},
   "source": [
    "# Audio Classification using Conformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dfbb12",
   "metadata": {},
   "source": [
    "This notebook provides a structure to train an audio classification model using Conformer as a feature extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e849f67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import librosa\n",
    "import numpy as np\n",
    "import sys\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93533dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/Users/jaewone/developer/tensorflow/baby-cry-classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae388a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from conformer.model import Conformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2767a25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_mel_spectrogram(file_path, n_mels=80):\n",
    "    y, sr = librosa.load(file_path, sr=16000)\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels)\n",
    "    log_mel_spectrogram = librosa.power_to_db(mel_spectrogram)\n",
    "    return log_mel_spectrogram.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a71fb73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jaewone/ENTER/envs/tf25/lib/python3.9/site-packages/numba/cpython/hashing.py:482: UserWarning: FNV hashing is not implemented in Numba. See PEP 456 https://www.python.org/dev/peps/pep-0456/ for rationale over not using FNV. Numba will continue to work, but hashes for built in types will be computed using siphash24. This will permit e.g. dictionaries to continue to behave as expected, however anything relying on the value of the hash opposed to hash as a derived property is likely to not work as expected.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70, 63, 80)\n",
      "(70,)\n"
     ]
    }
   ],
   "source": [
    "# Get data\n",
    "from trans_data import get_state_samples, split_dataset\n",
    "\n",
    "NUM_CLASSES = 7\n",
    "state_list = ['sleepy', 'uncomfortable', 'diaper', 'awake', 'sad', 'hug', 'hungry']\n",
    "class_map = {'sleepy': 0, 'uncomfortable': 1, 'diaper': 2, 'awake': 3, 'sad': 4, 'hug': 5, 'hungry': 6}\n",
    "\n",
    "# get wav file list\n",
    "file_list = get_state_samples('/Users/jaewone/developer/tensorflow/baby-cry-classification/data', n_extract=10)\n",
    "features = np.array([extract_mel_spectrogram(file) for file in file_list])\n",
    "labels = np.array([class_map[file.rsplit('/', 2)[1]] for file in file_list])\n",
    "print(features.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def26480",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bc3e127",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AudioClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(AudioClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class AudioClassificationModel(nn.Module):\n",
    "    def __init__(self, conformer, classifier):\n",
    "        super(AudioClassificationModel, self).__init__()\n",
    "        self.conformer = conformer\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def forward(self, inputs, input_lengths):\n",
    "        # Using the mean and max of the encoder outputs across time dimension for classification\n",
    "        encoder_outputs, _ = self.conformer(inputs, input_lengths)\n",
    "        encoder_outputs_mean = encoder_outputs.mean(dim=1)\n",
    "        encoder_outputs_max, _ = encoder_outputs.max(dim=1)\n",
    "        encoder_outputs_combined = torch.cat((encoder_outputs_mean, encoder_outputs_max), dim=1)\n",
    "        return self.classifier(encoder_outputs_combined)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b75dd5",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5279b635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n",
      "14\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Convert features and labels to torch tensors\n",
    "features_tensor = torch.tensor(features, dtype=torch.float32)\n",
    "labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "input_lengths_tensor = torch.tensor([f.shape[0] for f in features], dtype=torch.long)\n",
    "\n",
    "# Create a DataLoader\n",
    "dataset = TensorDataset(features_tensor, labels_tensor, input_lengths_tensor)\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [0.7, 0.2, 0.1])\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
    "print(len(train_dataset))\n",
    "print(len(val_dataset))\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "152a3e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "conformer_model = Conformer(num_classes=7)\n",
    "classifier = AudioClassifier(input_dim=1024, num_classes=7) \n",
    "audio_classification_model = AudioClassificationModel(conformer=conformer_model, classifier=classifier)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(audio_classification_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72579d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 training  : 100%|██████████| 25/25 [01:29<00:00,  3.58s/it]\n",
      "Epoch 1/1 validation: 100%|██████████| 7/7 [00:10<00:00,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.0281, Validation Loss: 2.1312\n",
      "Validation Accuracy: 7.14%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "def run(audio_classification_model, criterion):\n",
    "    num_epochs = 1\n",
    "    len_train_dataloader = len(train_loader)\n",
    "    len_val_dataloader = len(val_loader)\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        audio_classification_model.train()\n",
    "        train_loss = 0.0\n",
    "        with tqdm(total=len_train_dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs} training  \", position=0) as pbar:\n",
    "            for batch_features, batch_labels, batch_input_lengths in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = audio_classification_model(batch_features, batch_input_lengths)\n",
    "                loss = criterion(outputs, batch_labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "                pbar.update(1)\n",
    "\n",
    "        # Validation\n",
    "        audio_classification_model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with tqdm(total=len_val_dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs} validation\", position=0) as pbar:\n",
    "            with torch.no_grad():\n",
    "                for batch_features, batch_labels, batch_input_lengths in val_loader:\n",
    "                    outputs = audio_classification_model(batch_features, batch_input_lengths)\n",
    "                    loss = criterion(outputs, batch_labels)\n",
    "                    val_loss += loss.item()\n",
    "                    _, predicted = outputs.max(1)\n",
    "                    total += batch_labels.size(0)\n",
    "                    correct += predicted.eq(batch_labels).sum().item()\n",
    "                    pbar.update(1)\n",
    "\n",
    "        print(f\"Train Loss: {train_loss/len(train_loader):.4f}, Validation Loss: {val_loss/len(val_loader):.4f}\")\n",
    "        print(f\"Validation Accuracy: {100 * correct / total:.2f}%\\n\")\n",
    "    \n",
    "    return audio_classification_model, criterion\n",
    "\n",
    "audio_classification_model, criterion = run(audio_classification_model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c18351be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.0595\n",
      "Test Accuracy: 14.29%\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_features, batch_labels, batch_input_lengths in loader:\n",
    "            outputs = model(batch_features, batch_input_lengths)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += batch_labels.size(0)\n",
    "            correct += predicted.eq(batch_labels).sum().item()\n",
    "    return total_loss / len(loader), 100 * correct / total\n",
    "\n",
    "# Evaluating the model on the test dataset (after training)\n",
    "test_loss, test_accuracy = evaluate_model(audio_classification_model, test_loader, criterion)\n",
    "test_loss, test_accuracy\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fa9898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_checkpoint(epoch, model, optimizer, path):\n",
    "#     torch.save({\n",
    "#         'epoch': epoch,\n",
    "#         'model_state_dict': model.state_dict(),\n",
    "#         'optimizer_state_dict': optimizer.state_dict(),\n",
    "#     }, path)\n",
    "\n",
    "# best_val_loss = float('inf')\n",
    "# num_epochs = 10\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     train_loss = train(model, train_data, criterion, optimizer, device)\n",
    "#     val_loss = validate(model, val_data, criterion, device)\n",
    "\n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss}, Validation Loss: {val_loss}\")\n",
    "\n",
    "#     # 체크포인트 저장: 여기서는 검증 손실이 이전의 최고 값보다 낮을 때만 저장합니다.\n",
    "#     if val_loss < best_val_loss:\n",
    "#         best_val_loss = val_loss\n",
    "#         save_checkpoint(epoch, model, optimizer, \"/Users/jaewone/developer/tensorflow/baby-cry-classification/model/conformer2/best_model_checkpoint.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
