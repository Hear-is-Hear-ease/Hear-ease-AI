{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WaveNet 모델을 이용해 영아 울음소리 분류 모델을 만들어보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code: https://github.com/mjpyeon/wavenet-classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import librosa\n",
    "import sys\n",
    "from keras.layers import Input, Dense, Lambda, Flatten, Reshape, Activation, Dropout, Add, TimeDistributed, Multiply, Conv1D, Conv2D, MaxPooling1D, AveragePooling1D\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras import backend as K\n",
    "from keras import metrics\n",
    "from keras import optimizers\n",
    "from keras.callbacks import History, ModelCheckpoint\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/Users/jaewone/developer/tensorflow/baby-cry-classification')\n",
    "\n",
    "from utils.sound import *\n",
    "from utils.os import *\n",
    "from constant.os import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_path = os.path.join(main_path, 'sample_data')\n",
    "info_csv_path = os.path.join(main_path, 'sample_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 음성 파일 경로와 클래스 레이블 지정\n",
    "df = pd.read_csv(info_csv_path, index_col=0)\n",
    "audio_files = [os.path.join(sample_data_path, file) for file in df['file']]\n",
    "class_labels = df['state'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jaewone/ENTER/envs/tf25/lib/python3.9/site-packages/numba/cpython/hashing.py:482: UserWarning: FNV hashing is not implemented in Numba. See PEP 456 https://www.python.org/dev/peps/pep-0456/ for rationale over not using FNV. Numba will continue to work, but hashes for built in types will be computed using siphash24. This will permit e.g. dictionaries to continue to behave as expected, however anything relying on the value of the hash opposed to hash as a derived property is likely to not work as expected.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "X = [librosa.load(file, sr=16000)[0] for file in audio_files]\n",
    "y = class_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 80000)\n",
      "(2, 7)\n",
      "(2, 80000)\n"
     ]
    }
   ],
   "source": [
    "# 클래스 레이블 인코딩 및 원-핫 인코딩\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "num_classes = len(label_encoder.classes_)\n",
    "y_one_hot = tf.keras.utils.to_categorical(y_encoded, num_classes=num_classes)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_one_hot, test_size=0.2, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1)\n",
    "\n",
    "X_train = np.array(X_train[:20])\n",
    "X_test = np.array(X_test[:2])\n",
    "y_train = np.array(y_train[:20])\n",
    "y_test = np.array(y_test[:2])\n",
    "X_val = np.array(X_val[:2])\n",
    "y_val = np.array(y_val[:2])\n",
    "\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_test.shape)\n",
    "print(X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WaveNet model\n",
    "class WaveNetClassifier():\n",
    "    def __init__(self, input_shape, output_shape, kernel_size=2, dilation_depth=9, n_filters=40, load=False, load_dir='./'):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "          input_shape: (tuple) tuple of input shape. (e.g. If input is 6s raw waveform with sampling rate = 16kHz, (96000,) is the input_shape)\n",
    "          output_shape: (tuple)tuple of output shape. (e.g. If we want classify the signal into 100 classes, (100,) is the output_shape)\n",
    "          kernel_size: (integer) kernel size of convolution operations in residual blocks\n",
    "          dilation_depth: (integer) type total depth of residual blocks\n",
    "          n_filters: (integer) # of filters of convolution operations in residual blocks\n",
    "          load: (bool) load previous WaveNetClassifier or not\n",
    "          load_dir: (string) the directory where the previous model exists\n",
    "        \"\"\"\n",
    "        self.activation = 'softmax'\n",
    "        self.scale_ratio = 1\n",
    "\n",
    "        # save input info\n",
    "        if len(input_shape) == 1:\n",
    "            self.expand_dims = True\n",
    "        elif len(input_shape) == 2:\n",
    "            self.expand_dims = False\n",
    "        else:\n",
    "            print('ERROR: wrong input shape')\n",
    "            sys.exit()\n",
    "        self.input_shape = input_shape\n",
    "\n",
    "        # save output info\n",
    "        if len(output_shape) == 1:\n",
    "            self.time_distributed = False\n",
    "        elif len(output_shape) == 2:\n",
    "            self.time_distributed = True\n",
    "        else:\n",
    "            print('ERROR: wrong output shape')\n",
    "            sys.exit()\n",
    "        self.output_shape = output_shape\n",
    "\n",
    "        # save hyperparameters of WaveNet\n",
    "        self.kernel_size = kernel_size\n",
    "        self.dilation_depth = dilation_depth\n",
    "        self.n_filters = n_filters\n",
    "        self.manual_loss = None\n",
    "\n",
    "        if load is True:\n",
    "            self.model = load_model(\n",
    "                load_dir+\"saved_wavenet_clasifier.h5\", custom_objects={'tf': tf})\n",
    "            self.prev_history = pd.read_csv(\n",
    "                load_dir+'wavenet_classifier_training_history.csv')\n",
    "            self.start_idx = len(self.prev_history)\n",
    "            self.history = None\n",
    "        else:\n",
    "            self.model = self.construct_model()\n",
    "            self.start_idx = 0\n",
    "            self.history = None\n",
    "            self.prev_history = None\n",
    "\n",
    "    def residual_block(self, x, i):\n",
    "        tanh_out = Conv1D(self.n_filters,\n",
    "                          self.kernel_size,\n",
    "                          dilation_rate=self.kernel_size**i,\n",
    "                          padding='causal',\n",
    "                          name='dilated_conv_%d_tanh' % (\n",
    "                              self.kernel_size ** i),\n",
    "                          activation='tanh'\n",
    "                          )(x)\n",
    "        sigm_out = Conv1D(self.n_filters,\n",
    "                          self.kernel_size,\n",
    "                          dilation_rate=self.kernel_size**i,\n",
    "                          padding='causal',\n",
    "                          name='dilated_conv_%d_sigm' % (\n",
    "                              self.kernel_size ** i),\n",
    "                          activation='sigmoid'\n",
    "                          )(x)\n",
    "        z = Multiply(name='gated_activation_%d' % (i))([tanh_out, sigm_out])\n",
    "        skip = Conv1D(self.n_filters, 1, name='skip_%d' % (i))(z)\n",
    "        res = Add(name='residual_block_%d' % (i))([skip, x])\n",
    "        return res, skip\n",
    "\n",
    "    def construct_model(self):\n",
    "        x = Input(shape=self.input_shape, name='original_input')\n",
    "        if self.expand_dims == True:\n",
    "            x_reshaped = Reshape(self.input_shape + (1,),\n",
    "                                 name='reshaped_input')(x)\n",
    "        else:\n",
    "            x_reshaped = x\n",
    "        skip_connections = []\n",
    "        out = Conv1D(self.n_filters, 2, dilation_rate=1,\n",
    "                     padding='causal', name='dilated_conv_1')(x_reshaped)\n",
    "        for i in range(1, self.dilation_depth + 1):\n",
    "            out, skip = self.residual_block(out, i)\n",
    "            skip_connections.append(skip)\n",
    "        out = Add(name='skip_connections')(skip_connections)\n",
    "        out = Activation('relu')(out)\n",
    "        out = Conv1D(self.n_filters, 80, strides=1, padding='same',\n",
    "                     name='conv_5ms', activation='relu')(out)\n",
    "        out = AveragePooling1D(\n",
    "            80, padding='same', name='downsample_to_200Hz')(out)\n",
    "        if self.time_distributed:\n",
    "            # prev_len / x = target_len => x = prev_len / target_len\n",
    "            target_kernel_size = (int)(\n",
    "                self.input_shape[0] / 80 / self.output_shape[0])\n",
    "            out = Conv1D(self.n_filters, target_kernel_size, padding='same',\n",
    "                         name='conv_fit_to_target', activation='relu')(out)\n",
    "            out = Conv1D(\n",
    "                self.output_shape[1], target_kernel_size, padding='same', name='conv_final')(out)\n",
    "            out = AveragePooling1D(target_kernel_size, padding='same')(out)\n",
    "            out = TimeDistributed(Activation(self.activation))(out)\n",
    "        else:\n",
    "            out = Conv1D(self.n_filters, 100, padding='same',\n",
    "                         activation='relu', name='conv_500ms')(out)\n",
    "            out = Conv1D(self.output_shape[0], 100, padding='same',\n",
    "                         activation='relu', name='conv_500ms_target_shape')(out)\n",
    "            out = AveragePooling1D(100, padding='same',\n",
    "                                   name='downsample_to_2Hz')(out)\n",
    "            out = Conv1D(self.output_shape[0], (int)(\n",
    "                self.input_shape[0] / 8000), padding='same', name='final_conv')(out)\n",
    "            out = AveragePooling1D(\n",
    "                (int)(self.input_shape[0] / 8000), name='final_pooling')(out)\n",
    "            out = Reshape(self.output_shape)(out)\n",
    "            out = Activation(self.activation)(out)\n",
    "        if self.scale_ratio != 1:\n",
    "            out = Lambda(lambda x: x * self.scale_ratio,\n",
    "                         name='output_reshaped')(out)\n",
    "        model = Model(x, out)\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "    def get_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def add_loss(self, loss):\n",
    "        self.manual_loss = loss\n",
    "\n",
    "    def fit(self, X, Y, validation_data=None, epochs=100, batch_size=32, optimizer='adam', save=False, save_dir='./'):\n",
    "        # set default losses if not defined\n",
    "        if self.manual_loss is not None:\n",
    "            loss = self.manual_loss\n",
    "            metrics = None\n",
    "        else:\n",
    "            loss = 'categorical_crossentropy'\n",
    "            metrics = ['accuracy']\n",
    "\n",
    "        # set callback functions\n",
    "        if save:\n",
    "            saved = save_dir + \"saved_wavenet_clasifier.h5\"\n",
    "            hist = save_dir + 'wavenet_classifier_training_history.csv'\n",
    "            if validation_data is None:\n",
    "                checkpointer = ModelCheckpoint(\n",
    "                    filepath=saved, monitor='loss', verbose=1, save_best_only=True)\n",
    "            else:\n",
    "                checkpointer = ModelCheckpoint(\n",
    "                    filepath=saved, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "            history = History()\n",
    "            callbacks = [history, checkpointer]\n",
    "        else:\n",
    "            callbacks = None\n",
    "\n",
    "        # compile the model\n",
    "        self.model.compile(optimizer, loss, metrics)\n",
    "        try:\n",
    "            self.history = self.model.fit(X, Y, shuffle=True, batch_size=batch_size, epochs=epochs,\n",
    "                                          validation_data=validation_data, callbacks=callbacks, initial_epoch=self.start_idx)\n",
    "        except:\n",
    "            if save:\n",
    "                df = pd.DataFrame.from_dict(history.history)\n",
    "                df.to_csv(hist, encoding='utf-8', index=False)\n",
    "            raise\n",
    "            sys.exit()\n",
    "        return self.history\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-03 11:39:55.521427: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-08-03 11:39:55.521541: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " original_input (InputLayer)    [(None, 80000)]      0           []                               \n",
      "                                                                                                  \n",
      " reshaped_input (Reshape)       (None, 80000, 1)     0           ['original_input[0][0]']         \n",
      "                                                                                                  \n",
      " dilated_conv_1 (Conv1D)        (None, 80000, 40)    120         ['reshaped_input[0][0]']         \n",
      "                                                                                                  \n",
      " dilated_conv_2_tanh (Conv1D)   (None, 80000, 40)    3240        ['dilated_conv_1[0][0]']         \n",
      "                                                                                                  \n",
      " dilated_conv_2_sigm (Conv1D)   (None, 80000, 40)    3240        ['dilated_conv_1[0][0]']         \n",
      "                                                                                                  \n",
      " gated_activation_1 (Multiply)  (None, 80000, 40)    0           ['dilated_conv_2_tanh[0][0]',    \n",
      "                                                                  'dilated_conv_2_sigm[0][0]']    \n",
      "                                                                                                  \n",
      " skip_1 (Conv1D)                (None, 80000, 40)    1640        ['gated_activation_1[0][0]']     \n",
      "                                                                                                  \n",
      " residual_block_1 (Add)         (None, 80000, 40)    0           ['skip_1[0][0]',                 \n",
      "                                                                  'dilated_conv_1[0][0]']         \n",
      "                                                                                                  \n",
      " dilated_conv_4_tanh (Conv1D)   (None, 80000, 40)    3240        ['residual_block_1[0][0]']       \n",
      "                                                                                                  \n",
      " dilated_conv_4_sigm (Conv1D)   (None, 80000, 40)    3240        ['residual_block_1[0][0]']       \n",
      "                                                                                                  \n",
      " gated_activation_2 (Multiply)  (None, 80000, 40)    0           ['dilated_conv_4_tanh[0][0]',    \n",
      "                                                                  'dilated_conv_4_sigm[0][0]']    \n",
      "                                                                                                  \n",
      " skip_2 (Conv1D)                (None, 80000, 40)    1640        ['gated_activation_2[0][0]']     \n",
      "                                                                                                  \n",
      " residual_block_2 (Add)         (None, 80000, 40)    0           ['skip_2[0][0]',                 \n",
      "                                                                  'residual_block_1[0][0]']       \n",
      "                                                                                                  \n",
      " dilated_conv_8_tanh (Conv1D)   (None, 80000, 40)    3240        ['residual_block_2[0][0]']       \n",
      "                                                                                                  \n",
      " dilated_conv_8_sigm (Conv1D)   (None, 80000, 40)    3240        ['residual_block_2[0][0]']       \n",
      "                                                                                                  \n",
      " gated_activation_3 (Multiply)  (None, 80000, 40)    0           ['dilated_conv_8_tanh[0][0]',    \n",
      "                                                                  'dilated_conv_8_sigm[0][0]']    \n",
      "                                                                                                  \n",
      " skip_3 (Conv1D)                (None, 80000, 40)    1640        ['gated_activation_3[0][0]']     \n",
      "                                                                                                  \n",
      " residual_block_3 (Add)         (None, 80000, 40)    0           ['skip_3[0][0]',                 \n",
      "                                                                  'residual_block_2[0][0]']       \n",
      "                                                                                                  \n",
      " dilated_conv_16_tanh (Conv1D)  (None, 80000, 40)    3240        ['residual_block_3[0][0]']       \n",
      "                                                                                                  \n",
      " dilated_conv_16_sigm (Conv1D)  (None, 80000, 40)    3240        ['residual_block_3[0][0]']       \n",
      "                                                                                                  \n",
      " gated_activation_4 (Multiply)  (None, 80000, 40)    0           ['dilated_conv_16_tanh[0][0]',   \n",
      "                                                                  'dilated_conv_16_sigm[0][0]']   \n",
      "                                                                                                  \n",
      " skip_4 (Conv1D)                (None, 80000, 40)    1640        ['gated_activation_4[0][0]']     \n",
      "                                                                                                  \n",
      " residual_block_4 (Add)         (None, 80000, 40)    0           ['skip_4[0][0]',                 \n",
      "                                                                  'residual_block_3[0][0]']       \n",
      "                                                                                                  \n",
      " dilated_conv_32_tanh (Conv1D)  (None, 80000, 40)    3240        ['residual_block_4[0][0]']       \n",
      "                                                                                                  \n",
      " dilated_conv_32_sigm (Conv1D)  (None, 80000, 40)    3240        ['residual_block_4[0][0]']       \n",
      "                                                                                                  \n",
      " gated_activation_5 (Multiply)  (None, 80000, 40)    0           ['dilated_conv_32_tanh[0][0]',   \n",
      "                                                                  'dilated_conv_32_sigm[0][0]']   \n",
      "                                                                                                  \n",
      " skip_5 (Conv1D)                (None, 80000, 40)    1640        ['gated_activation_5[0][0]']     \n",
      "                                                                                                  \n",
      " residual_block_5 (Add)         (None, 80000, 40)    0           ['skip_5[0][0]',                 \n",
      "                                                                  'residual_block_4[0][0]']       \n",
      "                                                                                                  \n",
      " dilated_conv_64_tanh (Conv1D)  (None, 80000, 40)    3240        ['residual_block_5[0][0]']       \n",
      "                                                                                                  \n",
      " dilated_conv_64_sigm (Conv1D)  (None, 80000, 40)    3240        ['residual_block_5[0][0]']       \n",
      "                                                                                                  \n",
      " gated_activation_6 (Multiply)  (None, 80000, 40)    0           ['dilated_conv_64_tanh[0][0]',   \n",
      "                                                                  'dilated_conv_64_sigm[0][0]']   \n",
      "                                                                                                  \n",
      " skip_6 (Conv1D)                (None, 80000, 40)    1640        ['gated_activation_6[0][0]']     \n",
      "                                                                                                  \n",
      " residual_block_6 (Add)         (None, 80000, 40)    0           ['skip_6[0][0]',                 \n",
      "                                                                  'residual_block_5[0][0]']       \n",
      "                                                                                                  \n",
      " dilated_conv_128_tanh (Conv1D)  (None, 80000, 40)   3240        ['residual_block_6[0][0]']       \n",
      "                                                                                                  \n",
      " dilated_conv_128_sigm (Conv1D)  (None, 80000, 40)   3240        ['residual_block_6[0][0]']       \n",
      "                                                                                                  \n",
      " gated_activation_7 (Multiply)  (None, 80000, 40)    0           ['dilated_conv_128_tanh[0][0]',  \n",
      "                                                                  'dilated_conv_128_sigm[0][0]']  \n",
      "                                                                                                  \n",
      " skip_7 (Conv1D)                (None, 80000, 40)    1640        ['gated_activation_7[0][0]']     \n",
      "                                                                                                  \n",
      " residual_block_7 (Add)         (None, 80000, 40)    0           ['skip_7[0][0]',                 \n",
      "                                                                  'residual_block_6[0][0]']       \n",
      "                                                                                                  \n",
      " dilated_conv_256_tanh (Conv1D)  (None, 80000, 40)   3240        ['residual_block_7[0][0]']       \n",
      "                                                                                                  \n",
      " dilated_conv_256_sigm (Conv1D)  (None, 80000, 40)   3240        ['residual_block_7[0][0]']       \n",
      "                                                                                                  \n",
      " gated_activation_8 (Multiply)  (None, 80000, 40)    0           ['dilated_conv_256_tanh[0][0]',  \n",
      "                                                                  'dilated_conv_256_sigm[0][0]']  \n",
      "                                                                                                  \n",
      " skip_8 (Conv1D)                (None, 80000, 40)    1640        ['gated_activation_8[0][0]']     \n",
      "                                                                                                  \n",
      " residual_block_8 (Add)         (None, 80000, 40)    0           ['skip_8[0][0]',                 \n",
      "                                                                  'residual_block_7[0][0]']       \n",
      "                                                                                                  \n",
      " dilated_conv_512_tanh (Conv1D)  (None, 80000, 40)   3240        ['residual_block_8[0][0]']       \n",
      "                                                                                                  \n",
      " dilated_conv_512_sigm (Conv1D)  (None, 80000, 40)   3240        ['residual_block_8[0][0]']       \n",
      "                                                                                                  \n",
      " gated_activation_9 (Multiply)  (None, 80000, 40)    0           ['dilated_conv_512_tanh[0][0]',  \n",
      "                                                                  'dilated_conv_512_sigm[0][0]']  \n",
      "                                                                                                  \n",
      " skip_9 (Conv1D)                (None, 80000, 40)    1640        ['gated_activation_9[0][0]']     \n",
      "                                                                                                  \n",
      " skip_connections (Add)         (None, 80000, 40)    0           ['skip_1[0][0]',                 \n",
      "                                                                  'skip_2[0][0]',                 \n",
      "                                                                  'skip_3[0][0]',                 \n",
      "                                                                  'skip_4[0][0]',                 \n",
      "                                                                  'skip_5[0][0]',                 \n",
      "                                                                  'skip_6[0][0]',                 \n",
      "                                                                  'skip_7[0][0]',                 \n",
      "                                                                  'skip_8[0][0]',                 \n",
      "                                                                  'skip_9[0][0]']                 \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 80000, 40)    0           ['skip_connections[0][0]']       \n",
      "                                                                                                  \n",
      " conv_5ms (Conv1D)              (None, 80000, 40)    128040      ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " downsample_to_200Hz (AveragePo  (None, 1000, 40)    0           ['conv_5ms[0][0]']               \n",
      " oling1D)                                                                                         \n",
      "                                                                                                  \n",
      " conv_500ms (Conv1D)            (None, 1000, 40)     160040      ['downsample_to_200Hz[0][0]']    \n",
      "                                                                                                  \n",
      " conv_500ms_target_shape (Conv1  (None, 1000, 7)     28007       ['conv_500ms[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " downsample_to_2Hz (AveragePool  (None, 10, 7)       0           ['conv_500ms_target_shape[0][0]']\n",
      " ing1D)                                                                                           \n",
      "                                                                                                  \n",
      " final_conv (Conv1D)            (None, 10, 7)        497         ['downsample_to_2Hz[0][0]']      \n",
      "                                                                                                  \n",
      " final_pooling (AveragePooling1  (None, 1, 7)        0           ['final_conv[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 7)            0           ['final_pooling[0][0]']          \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 7)            0           ['reshape[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 389,784\n",
      "Trainable params: 389,784\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "model = WaveNetClassifier(\n",
    "    input_shape=(16000*5,),  # sample_rate * second\n",
    "    output_shape=(7,),       # label counts\n",
    "    kernel_size=2,\n",
    "    dilation_depth=9,\n",
    "    n_filters=40,\n",
    "    load=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-03 11:39:56.005248: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2023-08-03 11:39:57.399734: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - ETA: 0s - loss: 1.9453 - accuracy: 0.1000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-03 11:40:19.179041: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 2.95897, saving model to /Users/jaewone/developer/tensorflow/baby-cry-classification/model/historysaved_wavenet_clasifier.h5\n",
      "1/1 [==============================] - 24s 24s/step - loss: 1.9453 - accuracy: 0.1000 - val_loss: 2.9590 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.4087 - accuracy: 0.1000\n",
      "Epoch 2: val_loss improved from 2.95897 to 1.99747, saving model to /Users/jaewone/developer/tensorflow/baby-cry-classification/model/historysaved_wavenet_clasifier.h5\n",
      "1/1 [==============================] - 20s 20s/step - loss: 2.4087 - accuracy: 0.1000 - val_loss: 1.9975 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9332 - accuracy: 0.1000\n",
      "Epoch 3: val_loss improved from 1.99747 to 1.95632, saving model to /Users/jaewone/developer/tensorflow/baby-cry-classification/model/historysaved_wavenet_clasifier.h5\n",
      "1/1 [==============================] - 15s 15s/step - loss: 1.9332 - accuracy: 0.1000 - val_loss: 1.9563 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9436 - accuracy: 0.2500\n",
      "Epoch 4: val_loss improved from 1.95632 to 1.95308, saving model to /Users/jaewone/developer/tensorflow/baby-cry-classification/model/historysaved_wavenet_clasifier.h5\n",
      "1/1 [==============================] - 17s 17s/step - loss: 1.9436 - accuracy: 0.2500 - val_loss: 1.9531 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9435 - accuracy: 0.2500\n",
      "Epoch 5: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 16s 16s/step - loss: 1.9435 - accuracy: 0.2500 - val_loss: 1.9540 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9429 - accuracy: 0.2500\n",
      "Epoch 6: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 16s 16s/step - loss: 1.9429 - accuracy: 0.2500 - val_loss: 1.9582 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9417 - accuracy: 0.2500\n",
      "Epoch 7: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 20s 20s/step - loss: 1.9417 - accuracy: 0.2500 - val_loss: 1.9683 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9385 - accuracy: 0.2500\n",
      "Epoch 8: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 16s 16s/step - loss: 1.9385 - accuracy: 0.2500 - val_loss: 1.9909 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9312 - accuracy: 0.2500\n",
      "Epoch 9: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 16s 16s/step - loss: 1.9312 - accuracy: 0.2500 - val_loss: 2.0402 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9170 - accuracy: 0.2500\n",
      "Epoch 10: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 16s 16s/step - loss: 1.9170 - accuracy: 0.2500 - val_loss: 2.1465 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.8946 - accuracy: 0.2500\n",
      "Epoch 11: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 15s 15s/step - loss: 1.8946 - accuracy: 0.2500 - val_loss: 2.3633 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.8743 - accuracy: 0.2500\n",
      "Epoch 12: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 17s 17s/step - loss: 1.8743 - accuracy: 0.2500 - val_loss: 2.5102 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.8602 - accuracy: 0.2500\n",
      "Epoch 13: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 16s 16s/step - loss: 1.8602 - accuracy: 0.2500 - val_loss: 2.4185 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.8345 - accuracy: 0.2500\n",
      "Epoch 14: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 15s 15s/step - loss: 1.8345 - accuracy: 0.2500 - val_loss: 2.3021 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.8320 - accuracy: 0.2500\n",
      "Epoch 15: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 16s 16s/step - loss: 1.8320 - accuracy: 0.2500 - val_loss: 2.2881 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.8364 - accuracy: 0.3500\n",
      "Epoch 16: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 16s 16s/step - loss: 1.8364 - accuracy: 0.3500 - val_loss: 2.3376 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.8275 - accuracy: 0.2000\n",
      "Epoch 17: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 18s 18s/step - loss: 1.8275 - accuracy: 0.2000 - val_loss: 2.4296 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.8121 - accuracy: 0.3500\n",
      "Epoch 18: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 15s 15s/step - loss: 1.8121 - accuracy: 0.3500 - val_loss: 2.6062 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.7979 - accuracy: 0.3000\n",
      "Epoch 19: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 16s 16s/step - loss: 1.7979 - accuracy: 0.3000 - val_loss: 2.6943 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.7865 - accuracy: 0.3000\n",
      "Epoch 20: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 15s 15s/step - loss: 1.7865 - accuracy: 0.3000 - val_loss: 2.7375 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.7787 - accuracy: 0.2500\n",
      "Epoch 21: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 16s 16s/step - loss: 1.7787 - accuracy: 0.2500 - val_loss: 3.0887 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.7779 - accuracy: 0.2500\n",
      "Epoch 22: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 17s 17s/step - loss: 1.7779 - accuracy: 0.2500 - val_loss: 2.5827 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.7879 - accuracy: 0.3000\n",
      "Epoch 23: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 16s 16s/step - loss: 1.7879 - accuracy: 0.3000 - val_loss: 3.5013 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.7870 - accuracy: 0.3000\n",
      "Epoch 24: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 15s 15s/step - loss: 1.7870 - accuracy: 0.3000 - val_loss: 3.3850 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.7719 - accuracy: 0.3000\n",
      "Epoch 25: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 15s 15s/step - loss: 1.7719 - accuracy: 0.3000 - val_loss: 2.9708 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.7626 - accuracy: 0.4000\n",
      "Epoch 26: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 16s 16s/step - loss: 1.7626 - accuracy: 0.4000 - val_loss: 2.7392 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.7706 - accuracy: 0.2000\n",
      "Epoch 27: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 15s 15s/step - loss: 1.7706 - accuracy: 0.2000 - val_loss: 2.7710 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.7653 - accuracy: 0.2000\n",
      "Epoch 28: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 15s 15s/step - loss: 1.7653 - accuracy: 0.2000 - val_loss: 3.0363 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.7516 - accuracy: 0.2000\n",
      "Epoch 29: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 16s 16s/step - loss: 1.7516 - accuracy: 0.2000 - val_loss: 3.3802 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.7535 - accuracy: 0.2000\n",
      "Epoch 30: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 15s 15s/step - loss: 1.7535 - accuracy: 0.2000 - val_loss: 3.3839 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.7485 - accuracy: 0.2000\n",
      "Epoch 31: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 15s 15s/step - loss: 1.7485 - accuracy: 0.2000 - val_loss: 3.1163 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.7362 - accuracy: 0.2000\n",
      "Epoch 32: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 15s 15s/step - loss: 1.7362 - accuracy: 0.2000 - val_loss: 2.9054 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.7348 - accuracy: 0.4000\n",
      "Epoch 33: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 15s 15s/step - loss: 1.7348 - accuracy: 0.4000 - val_loss: 2.9105 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.7275 - accuracy: 0.4000\n",
      "Epoch 34: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 15s 15s/step - loss: 1.7275 - accuracy: 0.4000 - val_loss: 3.1042 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.7151 - accuracy: 0.3500\n",
      "Epoch 35: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 15s 15s/step - loss: 1.7151 - accuracy: 0.3500 - val_loss: 3.2765 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.7098 - accuracy: 0.3000\n",
      "Epoch 36: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 15s 15s/step - loss: 1.7098 - accuracy: 0.3000 - val_loss: 3.1922 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.7010 - accuracy: 0.3000\n",
      "Epoch 37: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 15s 15s/step - loss: 1.7010 - accuracy: 0.3000 - val_loss: 2.9661 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.6930 - accuracy: 0.3000\n",
      "Epoch 38: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 15s 15s/step - loss: 1.6930 - accuracy: 0.3000 - val_loss: 2.8490 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.6868 - accuracy: 0.3500\n",
      "Epoch 39: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 15s 15s/step - loss: 1.6868 - accuracy: 0.3500 - val_loss: 2.8987 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.6791 - accuracy: 0.3000\n",
      "Epoch 40: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 15s 15s/step - loss: 1.6791 - accuracy: 0.3000 - val_loss: 3.0412 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.6734 - accuracy: 0.3000\n",
      "Epoch 41: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 15s 15s/step - loss: 1.6734 - accuracy: 0.3000 - val_loss: 3.0755 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.6685 - accuracy: 0.3000\n",
      "Epoch 42: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 15s 15s/step - loss: 1.6685 - accuracy: 0.3000 - val_loss: 2.9036 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.6606 - accuracy: 0.3000\n",
      "Epoch 43: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 15s 15s/step - loss: 1.6606 - accuracy: 0.3000 - val_loss: 2.7396 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.6566 - accuracy: 0.4500\n",
      "Epoch 44: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 15s 15s/step - loss: 1.6566 - accuracy: 0.4500 - val_loss: 2.8233 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.6506 - accuracy: 0.4500\n",
      "Epoch 45: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 15s 15s/step - loss: 1.6506 - accuracy: 0.4500 - val_loss: 2.9394 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.6461 - accuracy: 0.4000\n",
      "Epoch 46: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 15s 15s/step - loss: 1.6461 - accuracy: 0.4000 - val_loss: 2.8961 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.6390 - accuracy: 0.4500\n",
      "Epoch 47: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 16s 16s/step - loss: 1.6390 - accuracy: 0.4500 - val_loss: 2.7861 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.6312 - accuracy: 0.4500\n",
      "Epoch 48: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 15s 15s/step - loss: 1.6312 - accuracy: 0.4500 - val_loss: 2.8079 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.6231 - accuracy: 0.4500\n",
      "Epoch 49: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 16s 16s/step - loss: 1.6231 - accuracy: 0.4500 - val_loss: 2.9735 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.6147 - accuracy: 0.4500\n",
      "Epoch 50: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 16s 16s/step - loss: 1.6147 - accuracy: 0.4500 - val_loss: 3.0156 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.6064 - accuracy: 0.4500\n",
      "Epoch 51: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 15s 15s/step - loss: 1.6064 - accuracy: 0.4500 - val_loss: 2.9125 - val_accuracy: 0.0000e+00\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.5977 - accuracy: 0.4500\n",
      "Epoch 52: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 15s 15s/step - loss: 1.5977 - accuracy: 0.4500 - val_loss: 3.0245 - val_accuracy: 0.0000e+00\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.5871 - accuracy: 0.4500\n",
      "Epoch 53: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 16s 16s/step - loss: 1.5871 - accuracy: 0.4500 - val_loss: 3.0950 - val_accuracy: 0.0000e+00\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.5767 - accuracy: 0.4500\n",
      "Epoch 54: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 15s 15s/step - loss: 1.5767 - accuracy: 0.4500 - val_loss: 2.9522 - val_accuracy: 0.0000e+00\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.5653 - accuracy: 0.4500\n",
      "Epoch 55: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 16s 16s/step - loss: 1.5653 - accuracy: 0.4500 - val_loss: 2.9785 - val_accuracy: 0.0000e+00\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.5530 - accuracy: 0.4000\n",
      "Epoch 56: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 16s 16s/step - loss: 1.5530 - accuracy: 0.4000 - val_loss: 3.1267 - val_accuracy: 0.0000e+00\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.5394 - accuracy: 0.4000\n",
      "Epoch 57: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 16s 16s/step - loss: 1.5394 - accuracy: 0.4000 - val_loss: 3.2256 - val_accuracy: 0.0000e+00\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.5247 - accuracy: 0.4500\n",
      "Epoch 58: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 15s 15s/step - loss: 1.5247 - accuracy: 0.4500 - val_loss: 3.1030 - val_accuracy: 0.0000e+00\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.5087 - accuracy: 0.4500\n",
      "Epoch 59: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 15s 15s/step - loss: 1.5087 - accuracy: 0.4500 - val_loss: 3.1763 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4914 - accuracy: 0.5000\n",
      "Epoch 60: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 15s 15s/step - loss: 1.4914 - accuracy: 0.5000 - val_loss: 3.3400 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4748 - accuracy: 0.5000\n",
      "Epoch 61: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 15s 15s/step - loss: 1.4748 - accuracy: 0.5000 - val_loss: 3.1054 - val_accuracy: 0.0000e+00\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4585 - accuracy: 0.5000\n",
      "Epoch 62: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 15s 15s/step - loss: 1.4585 - accuracy: 0.5000 - val_loss: 3.3138 - val_accuracy: 0.0000e+00\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4400 - accuracy: 0.5000\n",
      "Epoch 63: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 15s 15s/step - loss: 1.4400 - accuracy: 0.5000 - val_loss: 3.3939 - val_accuracy: 0.0000e+00\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4229 - accuracy: 0.5000\n",
      "Epoch 64: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 15s 15s/step - loss: 1.4229 - accuracy: 0.5000 - val_loss: 3.2350 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4058 - accuracy: 0.5000\n",
      "Epoch 65: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 15s 15s/step - loss: 1.4058 - accuracy: 0.5000 - val_loss: 3.6484 - val_accuracy: 0.0000e+00\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.3883 - accuracy: 0.5000\n",
      "Epoch 66: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 16s 16s/step - loss: 1.3883 - accuracy: 0.5000 - val_loss: 3.2155 - val_accuracy: 0.0000e+00\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.3728 - accuracy: 0.5000\n",
      "Epoch 67: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 15s 15s/step - loss: 1.3728 - accuracy: 0.5000 - val_loss: 4.1931 - val_accuracy: 0.0000e+00\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.3588 - accuracy: 0.5000\n",
      "Epoch 68: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 16s 16s/step - loss: 1.3588 - accuracy: 0.5000 - val_loss: 3.6499 - val_accuracy: 0.0000e+00\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.3294 - accuracy: 0.4500\n",
      "Epoch 69: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 15s 15s/step - loss: 1.3294 - accuracy: 0.4500 - val_loss: 3.9582 - val_accuracy: 0.0000e+00\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.3076 - accuracy: 0.5000\n",
      "Epoch 70: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 15s 15s/step - loss: 1.3076 - accuracy: 0.5000 - val_loss: 4.9719 - val_accuracy: 0.0000e+00\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.2940 - accuracy: 0.5000\n",
      "Epoch 71: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 15s 15s/step - loss: 1.2940 - accuracy: 0.5000 - val_loss: 4.3155 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.2833 - accuracy: 0.5500\n",
      "Epoch 72: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 16s 16s/step - loss: 1.2833 - accuracy: 0.5500 - val_loss: 7.8163 - val_accuracy: 0.0000e+00\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.3883 - accuracy: 0.5000\n",
      "Epoch 73: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 15s 15s/step - loss: 1.3883 - accuracy: 0.5000 - val_loss: 3.3049 - val_accuracy: 0.0000e+00\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.3860 - accuracy: 0.4500\n",
      "Epoch 74: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 15s 15s/step - loss: 1.3860 - accuracy: 0.4500 - val_loss: 4.0985 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.2701 - accuracy: 0.4500\n",
      "Epoch 75: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 15s 15s/step - loss: 1.2701 - accuracy: 0.4500 - val_loss: 5.8353 - val_accuracy: 0.0000e+00\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.2499 - accuracy: 0.5500\n",
      "Epoch 76: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 16s 16s/step - loss: 1.2499 - accuracy: 0.5500 - val_loss: 6.9144 - val_accuracy: 0.0000e+00\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.2851 - accuracy: 0.5500\n",
      "Epoch 77: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 16s 16s/step - loss: 1.2851 - accuracy: 0.5500 - val_loss: 4.6198 - val_accuracy: 0.0000e+00\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.2271 - accuracy: 0.4500\n",
      "Epoch 78: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 15s 15s/step - loss: 1.2271 - accuracy: 0.4500 - val_loss: 3.6298 - val_accuracy: 0.0000e+00\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.2242 - accuracy: 0.5000\n",
      "Epoch 79: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 16s 16s/step - loss: 1.2242 - accuracy: 0.5000 - val_loss: 3.9472 - val_accuracy: 0.0000e+00\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.2086 - accuracy: 0.6000\n",
      "Epoch 80: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 15s 15s/step - loss: 1.2086 - accuracy: 0.6000 - val_loss: 5.7405 - val_accuracy: 0.0000e+00\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.1708 - accuracy: 0.6500\n",
      "Epoch 81: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 15s 15s/step - loss: 1.1708 - accuracy: 0.6500 - val_loss: 7.4292 - val_accuracy: 0.0000e+00\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.1837 - accuracy: 0.6000\n",
      "Epoch 82: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 15s 15s/step - loss: 1.1837 - accuracy: 0.6000 - val_loss: 6.9820 - val_accuracy: 0.0000e+00\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.1486 - accuracy: 0.6000\n",
      "Epoch 83: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 15s 15s/step - loss: 1.1486 - accuracy: 0.6000 - val_loss: 5.5843 - val_accuracy: 0.0000e+00\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.1102 - accuracy: 0.6000\n",
      "Epoch 84: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 15s 15s/step - loss: 1.1102 - accuracy: 0.6000 - val_loss: 5.1153 - val_accuracy: 0.0000e+00\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.1117 - accuracy: 0.6500\n",
      "Epoch 85: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 16s 16s/step - loss: 1.1117 - accuracy: 0.6500 - val_loss: 6.0132 - val_accuracy: 0.0000e+00\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0723 - accuracy: 0.6000\n",
      "Epoch 86: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 16s 16s/step - loss: 1.0723 - accuracy: 0.6000 - val_loss: 7.4354 - val_accuracy: 0.0000e+00\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0504 - accuracy: 0.6000\n",
      "Epoch 87: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 15s 15s/step - loss: 1.0504 - accuracy: 0.6000 - val_loss: 8.5496 - val_accuracy: 0.0000e+00\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0494 - accuracy: 0.6000\n",
      "Epoch 88: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 15s 15s/step - loss: 1.0494 - accuracy: 0.6000 - val_loss: 8.7472 - val_accuracy: 0.0000e+00\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0086 - accuracy: 0.6000\n",
      "Epoch 89: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 16s 16s/step - loss: 1.0086 - accuracy: 0.6000 - val_loss: 8.3609 - val_accuracy: 0.0000e+00\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.9787 - accuracy: 0.6500\n",
      "Epoch 90: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 16s 16s/step - loss: 0.9787 - accuracy: 0.6500 - val_loss: 8.1018 - val_accuracy: 0.0000e+00\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.9562 - accuracy: 0.6500\n",
      "Epoch 91: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 16s 16s/step - loss: 0.9562 - accuracy: 0.6500 - val_loss: 9.2418 - val_accuracy: 0.0000e+00\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.9258 - accuracy: 0.6500\n",
      "Epoch 92: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 16s 16s/step - loss: 0.9258 - accuracy: 0.6500 - val_loss: 11.9534 - val_accuracy: 0.0000e+00\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.8925 - accuracy: 0.6500\n",
      "Epoch 93: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 16s 16s/step - loss: 0.8925 - accuracy: 0.6500 - val_loss: 13.5848 - val_accuracy: 0.0000e+00\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.8519 - accuracy: 0.6500\n",
      "Epoch 94: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 16s 16s/step - loss: 0.8519 - accuracy: 0.6500 - val_loss: 14.2621 - val_accuracy: 0.0000e+00\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.8190 - accuracy: 0.7000\n",
      "Epoch 95: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 16s 16s/step - loss: 0.8190 - accuracy: 0.7000 - val_loss: 17.6085 - val_accuracy: 0.0000e+00\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.8476 - accuracy: 0.7000\n",
      "Epoch 96: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 16s 16s/step - loss: 0.8476 - accuracy: 0.7000 - val_loss: 18.8545 - val_accuracy: 0.0000e+00\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.8088 - accuracy: 0.7500\n",
      "Epoch 97: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 16s 16s/step - loss: 0.8088 - accuracy: 0.7500 - val_loss: 23.6637 - val_accuracy: 0.0000e+00\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7248 - accuracy: 0.7500\n",
      "Epoch 98: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 16s 16s/step - loss: 0.7248 - accuracy: 0.7500 - val_loss: 20.6934 - val_accuracy: 0.0000e+00\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6883 - accuracy: 0.8000\n",
      "Epoch 99: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 16s 16s/step - loss: 0.6883 - accuracy: 0.8000 - val_loss: 22.3330 - val_accuracy: 0.0000e+00\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6571 - accuracy: 0.8000\n",
      "Epoch 100: val_loss did not improve from 1.95308\n",
      "1/1 [==============================] - 16s 16s/step - loss: 0.6571 - accuracy: 0.8000 - val_loss: 26.4274 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-03 12:06:09.648617: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=100,\n",
    "          batch_size=32, optimizer='adam', save=True, save_dir=os.path.join(main_path, 'model', 'history'))\n",
    "y_pred = model.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
