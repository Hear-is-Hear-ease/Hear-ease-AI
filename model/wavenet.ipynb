{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WaveNet 모델을 이용해 영아 울음소리 분류 모델을 만들어보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code: https://github.com/mjpyeon/wavenet-classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import librosa\n",
    "import sys\n",
    "from keras.layers import Input, Dense, Lambda, Flatten, Reshape, Activation, Dropout, Add, TimeDistributed, Multiply, Conv1D, Conv2D, MaxPooling1D, AveragePooling1D\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras import backend as K\n",
    "from keras import metrics\n",
    "from keras import optimizers\n",
    "from keras.callbacks import History, ModelCheckpoint\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/Users/jaewone/developer/tensorflow/baby-cry-classification')\n",
    "\n",
    "from utils.sound import *\n",
    "from utils.os import *\n",
    "from constant.os import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_path = os.path.join(main_path, 'sample_data')\n",
    "info_csv_path = os.path.join(main_path, 'sample_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 state 별로 n개만 추출한다.(데이터가 너무 많아 학습이 느리기 때문에 데이터 수를 줄인다.)\n",
    "df = pd.read_csv(info_csv_path, index_col=0)\n",
    "df = df.groupby('state').apply(lambda x: x.sample(n=6, random_state=42)).reset_index(drop=True)\n",
    "\n",
    "# 음성 파일 경로와 클래스 레이블 지정\n",
    "audio_files = [os.path.join(sample_data_path, file) for file in df['file']]\n",
    "class_labels = df['state'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jaewone/ENTER/envs/tf25/lib/python3.9/site-packages/numba/cpython/hashing.py:482: UserWarning: FNV hashing is not implemented in Numba. See PEP 456 https://www.python.org/dev/peps/pep-0456/ for rationale over not using FNV. Numba will continue to work, but hashes for built in types will be computed using siphash24. This will permit e.g. dictionaries to continue to behave as expected, however anything relying on the value of the hash opposed to hash as a derived property is likely to not work as expected.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "X = [librosa.load(file, sr=16000)[0] for file in audio_files]\n",
    "y = class_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26, 80000)\n",
      "(9, 7)\n",
      "(7, 80000)\n"
     ]
    }
   ],
   "source": [
    "# 클래스 레이블 인코딩 및 원-핫 인코딩\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "num_classes = len(label_encoder.classes_)\n",
    "y_one_hot = tf.keras.utils.to_categorical(y_encoded, num_classes=num_classes)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_one_hot, test_size=0.2, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "X_val = np.array(X_val)\n",
    "y_val = np.array(y_val)\n",
    "\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_test.shape)\n",
    "print(X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WaveNet model\n",
    "class WaveNetClassifier():\n",
    "    def __init__(self, input_shape, output_shape, kernel_size=2, dilation_depth=9, n_filters=40, load=False, load_dir='./'):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "          input_shape: (tuple) tuple of input shape. (e.g. If input is 6s raw waveform with sampling rate = 16kHz, (96000,) is the input_shape)\n",
    "          output_shape: (tuple)tuple of output shape. (e.g. If we want classify the signal into 100 classes, (100,) is the output_shape)\n",
    "          kernel_size: (integer) kernel size of convolution operations in residual blocks\n",
    "          dilation_depth: (integer) type total depth of residual blocks\n",
    "          n_filters: (integer) # of filters of convolution operations in residual blocks\n",
    "          load: (bool) load previous WaveNetClassifier or not\n",
    "          load_dir: (string) the directory where the previous model exists\n",
    "        \"\"\"\n",
    "        self.activation = 'softmax'\n",
    "        self.scale_ratio = 1\n",
    "\n",
    "        # save input info\n",
    "        if len(input_shape) == 1:\n",
    "            self.expand_dims = True\n",
    "        elif len(input_shape) == 2:\n",
    "            self.expand_dims = False\n",
    "        else:\n",
    "            print('ERROR: wrong input shape')\n",
    "            sys.exit()\n",
    "        self.input_shape = input_shape\n",
    "\n",
    "        # save output info\n",
    "        if len(output_shape) == 1:\n",
    "            self.time_distributed = False\n",
    "        elif len(output_shape) == 2:\n",
    "            self.time_distributed = True\n",
    "        else:\n",
    "            print('ERROR: wrong output shape')\n",
    "            sys.exit()\n",
    "        self.output_shape = output_shape\n",
    "\n",
    "        # save hyperparameters of WaveNet\n",
    "        self.kernel_size = kernel_size\n",
    "        self.dilation_depth = dilation_depth\n",
    "        self.n_filters = n_filters\n",
    "        self.manual_loss = None\n",
    "\n",
    "        if load is True:\n",
    "            self.model = load_model(\n",
    "                load_dir+\"saved_wavenet_clasifier.h5\", custom_objects={'tf': tf})\n",
    "            self.prev_history = pd.read_csv(\n",
    "                load_dir+'wavenet_classifier_training_history.csv')\n",
    "            self.start_idx = len(self.prev_history)\n",
    "            self.history = None\n",
    "        else:\n",
    "            self.model = self.construct_model()\n",
    "            self.start_idx = 0\n",
    "            self.history = None\n",
    "            self.prev_history = None\n",
    "\n",
    "    def residual_block(self, x, i):\n",
    "        tanh_out = Conv1D(self.n_filters,\n",
    "                          self.kernel_size,\n",
    "                          dilation_rate=self.kernel_size**i,\n",
    "                          padding='causal',\n",
    "                          name='dilated_conv_%d_tanh' % (\n",
    "                              self.kernel_size ** i),\n",
    "                          activation='tanh'\n",
    "                          )(x)\n",
    "        sigm_out = Conv1D(self.n_filters,\n",
    "                          self.kernel_size,\n",
    "                          dilation_rate=self.kernel_size**i,\n",
    "                          padding='causal',\n",
    "                          name='dilated_conv_%d_sigm' % (\n",
    "                              self.kernel_size ** i),\n",
    "                          activation='sigmoid'\n",
    "                          )(x)\n",
    "        z = Multiply(name='gated_activation_%d' % (i))([tanh_out, sigm_out])\n",
    "        skip = Conv1D(self.n_filters, 1, name='skip_%d' % (i))(z)\n",
    "        res = Add(name='residual_block_%d' % (i))([skip, x])\n",
    "        return res, skip\n",
    "\n",
    "    def construct_model(self):\n",
    "        x = Input(shape=self.input_shape, name='original_input')\n",
    "        if self.expand_dims == True:\n",
    "            x_reshaped = Reshape(self.input_shape + (1,),\n",
    "                                 name='reshaped_input')(x)\n",
    "        else:\n",
    "            x_reshaped = x\n",
    "        skip_connections = []\n",
    "        out = Conv1D(self.n_filters, 2, dilation_rate=1,\n",
    "                     padding='causal', name='dilated_conv_1')(x_reshaped)\n",
    "        for i in range(1, self.dilation_depth + 1):\n",
    "            out, skip = self.residual_block(out, i)\n",
    "            skip_connections.append(skip)\n",
    "        out = Add(name='skip_connections')(skip_connections)\n",
    "        out = Activation('relu')(out)\n",
    "        out = Conv1D(self.n_filters, 80, strides=1, padding='same',\n",
    "                     name='conv_5ms', activation='relu')(out)\n",
    "        out = AveragePooling1D(\n",
    "            80, padding='same', name='downsample_to_200Hz')(out)\n",
    "        if self.time_distributed:\n",
    "            # prev_len / x = target_len => x = prev_len / target_len\n",
    "            target_kernel_size = (int)(\n",
    "                self.input_shape[0] / 80 / self.output_shape[0])\n",
    "            out = Conv1D(self.n_filters, target_kernel_size, padding='same',\n",
    "                         name='conv_fit_to_target', activation='relu')(out)\n",
    "            out = Conv1D(\n",
    "                self.output_shape[1], target_kernel_size, padding='same', name='conv_final')(out)\n",
    "            out = AveragePooling1D(target_kernel_size, padding='same')(out)\n",
    "            out = TimeDistributed(Activation(self.activation))(out)\n",
    "        else:\n",
    "            out = Conv1D(self.n_filters, 100, padding='same',\n",
    "                         activation='relu', name='conv_500ms')(out)\n",
    "            out = Conv1D(self.output_shape[0], 100, padding='same',\n",
    "                         activation='relu', name='conv_500ms_target_shape')(out)\n",
    "            out = AveragePooling1D(100, padding='same',\n",
    "                                   name='downsample_to_2Hz')(out)\n",
    "            out = Conv1D(self.output_shape[0], (int)(\n",
    "                self.input_shape[0] / 8000), padding='same', name='final_conv')(out)\n",
    "            out = AveragePooling1D(\n",
    "                (int)(self.input_shape[0] / 8000), name='final_pooling')(out)\n",
    "            out = Reshape(self.output_shape)(out)\n",
    "            out = Activation(self.activation)(out)\n",
    "        if self.scale_ratio != 1:\n",
    "            out = Lambda(lambda x: x * self.scale_ratio,\n",
    "                         name='output_reshaped')(out)\n",
    "        model = Model(x, out)\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "    def get_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def add_loss(self, loss):\n",
    "        self.manual_loss = loss\n",
    "\n",
    "    def fit(self, X, Y, validation_data=None, epochs=100, batch_size=32, optimizer='adam', save=False, save_dir='./'):\n",
    "        # set default losses if not defined\n",
    "        if self.manual_loss is not None:\n",
    "            loss = self.manual_loss\n",
    "            metrics = None\n",
    "        else:\n",
    "            loss = 'categorical_crossentropy'\n",
    "            metrics = ['accuracy']\n",
    "\n",
    "        # set callback functions\n",
    "        if save:\n",
    "            saved = save_dir + \"saved_wavenet_clasifier.h5\"\n",
    "            hist = save_dir + 'wavenet_classifier_training_history.csv'\n",
    "            if validation_data is None:\n",
    "                checkpointer = ModelCheckpoint(\n",
    "                    filepath=saved, monitor='loss', verbose=1, save_best_only=True)\n",
    "            else:\n",
    "                checkpointer = ModelCheckpoint(\n",
    "                    filepath=saved, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "            history = History()\n",
    "            callbacks = [history, checkpointer]\n",
    "        else:\n",
    "            callbacks = None\n",
    "\n",
    "        # compile the model\n",
    "        self.model.compile(optimizer, loss, metrics)\n",
    "        try:\n",
    "            self.history = self.model.fit(X, Y, shuffle=True, batch_size=batch_size, epochs=epochs,\n",
    "                                          validation_data=validation_data, callbacks=callbacks, initial_epoch=self.start_idx)\n",
    "        except:\n",
    "            if save:\n",
    "                df = pd.DataFrame.from_dict(history.history)\n",
    "                df.to_csv(hist, encoding='utf-8', index=False)\n",
    "            raise\n",
    "            sys.exit()\n",
    "        return self.history\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-03 13:13:35.225785: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-08-03 13:13:35.225900: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " original_input (InputLayer)    [(None, 80000)]      0           []                               \n",
      "                                                                                                  \n",
      " reshaped_input (Reshape)       (None, 80000, 1)     0           ['original_input[0][0]']         \n",
      "                                                                                                  \n",
      " dilated_conv_1 (Conv1D)        (None, 80000, 40)    120         ['reshaped_input[0][0]']         \n",
      "                                                                                                  \n",
      " dilated_conv_2_tanh (Conv1D)   (None, 80000, 40)    3240        ['dilated_conv_1[0][0]']         \n",
      "                                                                                                  \n",
      " dilated_conv_2_sigm (Conv1D)   (None, 80000, 40)    3240        ['dilated_conv_1[0][0]']         \n",
      "                                                                                                  \n",
      " gated_activation_1 (Multiply)  (None, 80000, 40)    0           ['dilated_conv_2_tanh[0][0]',    \n",
      "                                                                  'dilated_conv_2_sigm[0][0]']    \n",
      "                                                                                                  \n",
      " skip_1 (Conv1D)                (None, 80000, 40)    1640        ['gated_activation_1[0][0]']     \n",
      "                                                                                                  \n",
      " residual_block_1 (Add)         (None, 80000, 40)    0           ['skip_1[0][0]',                 \n",
      "                                                                  'dilated_conv_1[0][0]']         \n",
      "                                                                                                  \n",
      " dilated_conv_4_tanh (Conv1D)   (None, 80000, 40)    3240        ['residual_block_1[0][0]']       \n",
      "                                                                                                  \n",
      " dilated_conv_4_sigm (Conv1D)   (None, 80000, 40)    3240        ['residual_block_1[0][0]']       \n",
      "                                                                                                  \n",
      " gated_activation_2 (Multiply)  (None, 80000, 40)    0           ['dilated_conv_4_tanh[0][0]',    \n",
      "                                                                  'dilated_conv_4_sigm[0][0]']    \n",
      "                                                                                                  \n",
      " skip_2 (Conv1D)                (None, 80000, 40)    1640        ['gated_activation_2[0][0]']     \n",
      "                                                                                                  \n",
      " residual_block_2 (Add)         (None, 80000, 40)    0           ['skip_2[0][0]',                 \n",
      "                                                                  'residual_block_1[0][0]']       \n",
      "                                                                                                  \n",
      " dilated_conv_8_tanh (Conv1D)   (None, 80000, 40)    3240        ['residual_block_2[0][0]']       \n",
      "                                                                                                  \n",
      " dilated_conv_8_sigm (Conv1D)   (None, 80000, 40)    3240        ['residual_block_2[0][0]']       \n",
      "                                                                                                  \n",
      " gated_activation_3 (Multiply)  (None, 80000, 40)    0           ['dilated_conv_8_tanh[0][0]',    \n",
      "                                                                  'dilated_conv_8_sigm[0][0]']    \n",
      "                                                                                                  \n",
      " skip_3 (Conv1D)                (None, 80000, 40)    1640        ['gated_activation_3[0][0]']     \n",
      "                                                                                                  \n",
      " residual_block_3 (Add)         (None, 80000, 40)    0           ['skip_3[0][0]',                 \n",
      "                                                                  'residual_block_2[0][0]']       \n",
      "                                                                                                  \n",
      " dilated_conv_16_tanh (Conv1D)  (None, 80000, 40)    3240        ['residual_block_3[0][0]']       \n",
      "                                                                                                  \n",
      " dilated_conv_16_sigm (Conv1D)  (None, 80000, 40)    3240        ['residual_block_3[0][0]']       \n",
      "                                                                                                  \n",
      " gated_activation_4 (Multiply)  (None, 80000, 40)    0           ['dilated_conv_16_tanh[0][0]',   \n",
      "                                                                  'dilated_conv_16_sigm[0][0]']   \n",
      "                                                                                                  \n",
      " skip_4 (Conv1D)                (None, 80000, 40)    1640        ['gated_activation_4[0][0]']     \n",
      "                                                                                                  \n",
      " residual_block_4 (Add)         (None, 80000, 40)    0           ['skip_4[0][0]',                 \n",
      "                                                                  'residual_block_3[0][0]']       \n",
      "                                                                                                  \n",
      " dilated_conv_32_tanh (Conv1D)  (None, 80000, 40)    3240        ['residual_block_4[0][0]']       \n",
      "                                                                                                  \n",
      " dilated_conv_32_sigm (Conv1D)  (None, 80000, 40)    3240        ['residual_block_4[0][0]']       \n",
      "                                                                                                  \n",
      " gated_activation_5 (Multiply)  (None, 80000, 40)    0           ['dilated_conv_32_tanh[0][0]',   \n",
      "                                                                  'dilated_conv_32_sigm[0][0]']   \n",
      "                                                                                                  \n",
      " skip_5 (Conv1D)                (None, 80000, 40)    1640        ['gated_activation_5[0][0]']     \n",
      "                                                                                                  \n",
      " residual_block_5 (Add)         (None, 80000, 40)    0           ['skip_5[0][0]',                 \n",
      "                                                                  'residual_block_4[0][0]']       \n",
      "                                                                                                  \n",
      " dilated_conv_64_tanh (Conv1D)  (None, 80000, 40)    3240        ['residual_block_5[0][0]']       \n",
      "                                                                                                  \n",
      " dilated_conv_64_sigm (Conv1D)  (None, 80000, 40)    3240        ['residual_block_5[0][0]']       \n",
      "                                                                                                  \n",
      " gated_activation_6 (Multiply)  (None, 80000, 40)    0           ['dilated_conv_64_tanh[0][0]',   \n",
      "                                                                  'dilated_conv_64_sigm[0][0]']   \n",
      "                                                                                                  \n",
      " skip_6 (Conv1D)                (None, 80000, 40)    1640        ['gated_activation_6[0][0]']     \n",
      "                                                                                                  \n",
      " residual_block_6 (Add)         (None, 80000, 40)    0           ['skip_6[0][0]',                 \n",
      "                                                                  'residual_block_5[0][0]']       \n",
      "                                                                                                  \n",
      " dilated_conv_128_tanh (Conv1D)  (None, 80000, 40)   3240        ['residual_block_6[0][0]']       \n",
      "                                                                                                  \n",
      " dilated_conv_128_sigm (Conv1D)  (None, 80000, 40)   3240        ['residual_block_6[0][0]']       \n",
      "                                                                                                  \n",
      " gated_activation_7 (Multiply)  (None, 80000, 40)    0           ['dilated_conv_128_tanh[0][0]',  \n",
      "                                                                  'dilated_conv_128_sigm[0][0]']  \n",
      "                                                                                                  \n",
      " skip_7 (Conv1D)                (None, 80000, 40)    1640        ['gated_activation_7[0][0]']     \n",
      "                                                                                                  \n",
      " residual_block_7 (Add)         (None, 80000, 40)    0           ['skip_7[0][0]',                 \n",
      "                                                                  'residual_block_6[0][0]']       \n",
      "                                                                                                  \n",
      " dilated_conv_256_tanh (Conv1D)  (None, 80000, 40)   3240        ['residual_block_7[0][0]']       \n",
      "                                                                                                  \n",
      " dilated_conv_256_sigm (Conv1D)  (None, 80000, 40)   3240        ['residual_block_7[0][0]']       \n",
      "                                                                                                  \n",
      " gated_activation_8 (Multiply)  (None, 80000, 40)    0           ['dilated_conv_256_tanh[0][0]',  \n",
      "                                                                  'dilated_conv_256_sigm[0][0]']  \n",
      "                                                                                                  \n",
      " skip_8 (Conv1D)                (None, 80000, 40)    1640        ['gated_activation_8[0][0]']     \n",
      "                                                                                                  \n",
      " residual_block_8 (Add)         (None, 80000, 40)    0           ['skip_8[0][0]',                 \n",
      "                                                                  'residual_block_7[0][0]']       \n",
      "                                                                                                  \n",
      " dilated_conv_512_tanh (Conv1D)  (None, 80000, 40)   3240        ['residual_block_8[0][0]']       \n",
      "                                                                                                  \n",
      " dilated_conv_512_sigm (Conv1D)  (None, 80000, 40)   3240        ['residual_block_8[0][0]']       \n",
      "                                                                                                  \n",
      " gated_activation_9 (Multiply)  (None, 80000, 40)    0           ['dilated_conv_512_tanh[0][0]',  \n",
      "                                                                  'dilated_conv_512_sigm[0][0]']  \n",
      "                                                                                                  \n",
      " skip_9 (Conv1D)                (None, 80000, 40)    1640        ['gated_activation_9[0][0]']     \n",
      "                                                                                                  \n",
      " skip_connections (Add)         (None, 80000, 40)    0           ['skip_1[0][0]',                 \n",
      "                                                                  'skip_2[0][0]',                 \n",
      "                                                                  'skip_3[0][0]',                 \n",
      "                                                                  'skip_4[0][0]',                 \n",
      "                                                                  'skip_5[0][0]',                 \n",
      "                                                                  'skip_6[0][0]',                 \n",
      "                                                                  'skip_7[0][0]',                 \n",
      "                                                                  'skip_8[0][0]',                 \n",
      "                                                                  'skip_9[0][0]']                 \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 80000, 40)    0           ['skip_connections[0][0]']       \n",
      "                                                                                                  \n",
      " conv_5ms (Conv1D)              (None, 80000, 40)    128040      ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " downsample_to_200Hz (AveragePo  (None, 1000, 40)    0           ['conv_5ms[0][0]']               \n",
      " oling1D)                                                                                         \n",
      "                                                                                                  \n",
      " conv_500ms (Conv1D)            (None, 1000, 40)     160040      ['downsample_to_200Hz[0][0]']    \n",
      "                                                                                                  \n",
      " conv_500ms_target_shape (Conv1  (None, 1000, 7)     28007       ['conv_500ms[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " downsample_to_2Hz (AveragePool  (None, 10, 7)       0           ['conv_500ms_target_shape[0][0]']\n",
      " ing1D)                                                                                           \n",
      "                                                                                                  \n",
      " final_conv (Conv1D)            (None, 10, 7)        497         ['downsample_to_2Hz[0][0]']      \n",
      "                                                                                                  \n",
      " final_pooling (AveragePooling1  (None, 1, 7)        0           ['final_conv[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 7)            0           ['final_pooling[0][0]']          \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 7)            0           ['reshape[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 389,784\n",
      "Trainable params: 389,784\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "model = WaveNetClassifier(\n",
    "    input_shape=(16000*5,),  # sample_rate * second\n",
    "    output_shape=(7,),       # label counts\n",
    "    kernel_size=2,\n",
    "    dilation_depth=9,\n",
    "    n_filters=40,\n",
    "    load=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-03 13:13:35.767746: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2023-08-03 13:13:37.132567: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - ETA: 0s - loss: 1.9518 - accuracy: 0.0769"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-03 13:14:13.903460: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 1.85323, saving model to /Users/jaewone/developer/tensorflow/baby-cry-classification/model/historysaved_wavenet_clasifier.h5\n",
      "1/1 [==============================] - 40s 40s/step - loss: 1.9518 - accuracy: 0.0769 - val_loss: 1.8532 - val_accuracy: 0.1429\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9668 - accuracy: 0.1538\n",
      "Epoch 2: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 38s 38s/step - loss: 1.9668 - accuracy: 0.1538 - val_loss: 1.9473 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9454 - accuracy: 0.2308\n",
      "Epoch 3: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 34s 34s/step - loss: 1.9454 - accuracy: 0.2308 - val_loss: 1.9496 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9453 - accuracy: 0.1538\n",
      "Epoch 4: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 34s 34s/step - loss: 1.9453 - accuracy: 0.1538 - val_loss: 1.9703 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9451 - accuracy: 0.1538\n",
      "Epoch 5: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 31s 31s/step - loss: 1.9451 - accuracy: 0.1538 - val_loss: 1.9502 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9447 - accuracy: 0.1538\n",
      "Epoch 6: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 32s 32s/step - loss: 1.9447 - accuracy: 0.1538 - val_loss: 1.9541 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9478 - accuracy: 0.1538\n",
      "Epoch 7: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 30s 30s/step - loss: 1.9478 - accuracy: 0.1538 - val_loss: 1.9517 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9443 - accuracy: 0.1538\n",
      "Epoch 8: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 32s 32s/step - loss: 1.9443 - accuracy: 0.1538 - val_loss: 1.9523 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9441 - accuracy: 0.2308\n",
      "Epoch 9: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 32s 32s/step - loss: 1.9441 - accuracy: 0.2308 - val_loss: 1.9531 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9439 - accuracy: 0.2308\n",
      "Epoch 10: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 31s 31s/step - loss: 1.9439 - accuracy: 0.2308 - val_loss: 1.9539 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9437 - accuracy: 0.2308\n",
      "Epoch 11: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 31s 31s/step - loss: 1.9437 - accuracy: 0.2308 - val_loss: 1.9547 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9434 - accuracy: 0.2308\n",
      "Epoch 12: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 31s 31s/step - loss: 1.9434 - accuracy: 0.2308 - val_loss: 1.9554 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9432 - accuracy: 0.2308\n",
      "Epoch 13: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 30s 30s/step - loss: 1.9432 - accuracy: 0.2308 - val_loss: 1.9562 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9430 - accuracy: 0.2308\n",
      "Epoch 14: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 30s 30s/step - loss: 1.9430 - accuracy: 0.2308 - val_loss: 1.9570 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9428 - accuracy: 0.2308\n",
      "Epoch 15: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 31s 31s/step - loss: 1.9428 - accuracy: 0.2308 - val_loss: 1.9578 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9426 - accuracy: 0.2308\n",
      "Epoch 16: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 33s 33s/step - loss: 1.9426 - accuracy: 0.2308 - val_loss: 1.9586 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9423 - accuracy: 0.2308\n",
      "Epoch 17: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 30s 30s/step - loss: 1.9423 - accuracy: 0.2308 - val_loss: 1.9594 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9421 - accuracy: 0.2308\n",
      "Epoch 18: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 33s 33s/step - loss: 1.9421 - accuracy: 0.2308 - val_loss: 1.9602 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9419 - accuracy: 0.2308\n",
      "Epoch 19: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 33s 33s/step - loss: 1.9419 - accuracy: 0.2308 - val_loss: 1.9610 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9417 - accuracy: 0.2308\n",
      "Epoch 20: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 31s 31s/step - loss: 1.9417 - accuracy: 0.2308 - val_loss: 1.9618 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9415 - accuracy: 0.2308\n",
      "Epoch 21: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 33s 33s/step - loss: 1.9415 - accuracy: 0.2308 - val_loss: 1.9626 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9412 - accuracy: 0.2308\n",
      "Epoch 22: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 32s 32s/step - loss: 1.9412 - accuracy: 0.2308 - val_loss: 1.9634 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9410 - accuracy: 0.2308\n",
      "Epoch 23: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 32s 32s/step - loss: 1.9410 - accuracy: 0.2308 - val_loss: 1.9642 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9408 - accuracy: 0.2308\n",
      "Epoch 24: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 32s 32s/step - loss: 1.9408 - accuracy: 0.2308 - val_loss: 1.9650 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9406 - accuracy: 0.2308\n",
      "Epoch 25: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 33s 33s/step - loss: 1.9406 - accuracy: 0.2308 - val_loss: 1.9658 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9404 - accuracy: 0.2308\n",
      "Epoch 26: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 33s 33s/step - loss: 1.9404 - accuracy: 0.2308 - val_loss: 1.9666 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9402 - accuracy: 0.2308\n",
      "Epoch 27: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 32s 32s/step - loss: 1.9402 - accuracy: 0.2308 - val_loss: 1.9674 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9400 - accuracy: 0.2308\n",
      "Epoch 28: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 33s 33s/step - loss: 1.9400 - accuracy: 0.2308 - val_loss: 1.9682 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9398 - accuracy: 0.2308\n",
      "Epoch 29: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 30s 30s/step - loss: 1.9398 - accuracy: 0.2308 - val_loss: 1.9690 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9396 - accuracy: 0.2308\n",
      "Epoch 30: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 31s 31s/step - loss: 1.9396 - accuracy: 0.2308 - val_loss: 1.9698 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9394 - accuracy: 0.2308\n",
      "Epoch 31: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 32s 32s/step - loss: 1.9394 - accuracy: 0.2308 - val_loss: 1.9706 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9392 - accuracy: 0.2308\n",
      "Epoch 32: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 30s 30s/step - loss: 1.9392 - accuracy: 0.2308 - val_loss: 1.9714 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9390 - accuracy: 0.2308\n",
      "Epoch 33: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 35s 35s/step - loss: 1.9390 - accuracy: 0.2308 - val_loss: 1.9722 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9387 - accuracy: 0.2308\n",
      "Epoch 34: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 34s 34s/step - loss: 1.9387 - accuracy: 0.2308 - val_loss: 1.9730 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9385 - accuracy: 0.2308\n",
      "Epoch 35: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 35s 35s/step - loss: 1.9385 - accuracy: 0.2308 - val_loss: 1.9738 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9383 - accuracy: 0.2308\n",
      "Epoch 36: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 37s 37s/step - loss: 1.9383 - accuracy: 0.2308 - val_loss: 1.9746 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9381 - accuracy: 0.2308\n",
      "Epoch 37: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 31s 31s/step - loss: 1.9381 - accuracy: 0.2308 - val_loss: 1.9754 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9379 - accuracy: 0.2308\n",
      "Epoch 38: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 35s 35s/step - loss: 1.9379 - accuracy: 0.2308 - val_loss: 1.9762 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9377 - accuracy: 0.2308\n",
      "Epoch 39: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 35s 35s/step - loss: 1.9377 - accuracy: 0.2308 - val_loss: 1.9769 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9376 - accuracy: 0.2308\n",
      "Epoch 40: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 33s 33s/step - loss: 1.9376 - accuracy: 0.2308 - val_loss: 1.9777 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9374 - accuracy: 0.2308\n",
      "Epoch 41: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 35s 35s/step - loss: 1.9374 - accuracy: 0.2308 - val_loss: 1.9785 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9372 - accuracy: 0.2308\n",
      "Epoch 42: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 34s 34s/step - loss: 1.9372 - accuracy: 0.2308 - val_loss: 1.9793 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9370 - accuracy: 0.2308\n",
      "Epoch 43: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 32s 32s/step - loss: 1.9370 - accuracy: 0.2308 - val_loss: 1.9801 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9368 - accuracy: 0.2308\n",
      "Epoch 44: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 37s 37s/step - loss: 1.9368 - accuracy: 0.2308 - val_loss: 1.9809 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9366 - accuracy: 0.2308\n",
      "Epoch 45: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 31s 31s/step - loss: 1.9366 - accuracy: 0.2308 - val_loss: 1.9817 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9364 - accuracy: 0.2308\n",
      "Epoch 46: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 30s 30s/step - loss: 1.9364 - accuracy: 0.2308 - val_loss: 1.9824 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9362 - accuracy: 0.2308\n",
      "Epoch 47: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 31s 31s/step - loss: 1.9362 - accuracy: 0.2308 - val_loss: 1.9832 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9360 - accuracy: 0.2308\n",
      "Epoch 48: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 31s 31s/step - loss: 1.9360 - accuracy: 0.2308 - val_loss: 1.9840 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9358 - accuracy: 0.2308\n",
      "Epoch 49: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 30s 30s/step - loss: 1.9358 - accuracy: 0.2308 - val_loss: 1.9848 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9356 - accuracy: 0.2308\n",
      "Epoch 50: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 30s 30s/step - loss: 1.9356 - accuracy: 0.2308 - val_loss: 1.9856 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9355 - accuracy: 0.2308\n",
      "Epoch 51: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 32s 32s/step - loss: 1.9355 - accuracy: 0.2308 - val_loss: 1.9863 - val_accuracy: 0.0000e+00\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9353 - accuracy: 0.2308\n",
      "Epoch 52: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 31s 31s/step - loss: 1.9353 - accuracy: 0.2308 - val_loss: 1.9871 - val_accuracy: 0.0000e+00\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9351 - accuracy: 0.2308\n",
      "Epoch 53: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 32s 32s/step - loss: 1.9351 - accuracy: 0.2308 - val_loss: 1.9879 - val_accuracy: 0.0000e+00\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9349 - accuracy: 0.2308\n",
      "Epoch 54: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 32s 32s/step - loss: 1.9349 - accuracy: 0.2308 - val_loss: 1.9886 - val_accuracy: 0.0000e+00\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9347 - accuracy: 0.2308\n",
      "Epoch 55: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 32s 32s/step - loss: 1.9347 - accuracy: 0.2308 - val_loss: 1.9894 - val_accuracy: 0.0000e+00\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9346 - accuracy: 0.2308\n",
      "Epoch 56: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 31s 31s/step - loss: 1.9346 - accuracy: 0.2308 - val_loss: 1.9902 - val_accuracy: 0.0000e+00\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9344 - accuracy: 0.2308\n",
      "Epoch 57: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 30s 30s/step - loss: 1.9344 - accuracy: 0.2308 - val_loss: 1.9909 - val_accuracy: 0.0000e+00\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9342 - accuracy: 0.2308\n",
      "Epoch 58: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 33s 33s/step - loss: 1.9342 - accuracy: 0.2308 - val_loss: 1.9917 - val_accuracy: 0.0000e+00\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9340 - accuracy: 0.2308\n",
      "Epoch 59: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 31s 31s/step - loss: 1.9340 - accuracy: 0.2308 - val_loss: 1.9924 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9338 - accuracy: 0.2308\n",
      "Epoch 60: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 32s 32s/step - loss: 1.9338 - accuracy: 0.2308 - val_loss: 1.9932 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9337 - accuracy: 0.2308\n",
      "Epoch 61: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 31s 31s/step - loss: 1.9337 - accuracy: 0.2308 - val_loss: 1.9939 - val_accuracy: 0.0000e+00\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9335 - accuracy: 0.2308\n",
      "Epoch 62: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 30s 30s/step - loss: 1.9335 - accuracy: 0.2308 - val_loss: 1.9947 - val_accuracy: 0.0000e+00\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9333 - accuracy: 0.2308\n",
      "Epoch 63: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 33s 33s/step - loss: 1.9333 - accuracy: 0.2308 - val_loss: 1.9954 - val_accuracy: 0.0000e+00\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9331 - accuracy: 0.2308\n",
      "Epoch 64: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 31s 31s/step - loss: 1.9331 - accuracy: 0.2308 - val_loss: 1.9962 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9330 - accuracy: 0.2308\n",
      "Epoch 65: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 32s 32s/step - loss: 1.9330 - accuracy: 0.2308 - val_loss: 1.9969 - val_accuracy: 0.0000e+00\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9328 - accuracy: 0.2308\n",
      "Epoch 66: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 30s 30s/step - loss: 1.9328 - accuracy: 0.2308 - val_loss: 1.9977 - val_accuracy: 0.0000e+00\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9326 - accuracy: 0.2308\n",
      "Epoch 67: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 31s 31s/step - loss: 1.9326 - accuracy: 0.2308 - val_loss: 1.9984 - val_accuracy: 0.0000e+00\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9325 - accuracy: 0.2308\n",
      "Epoch 68: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 31s 31s/step - loss: 1.9325 - accuracy: 0.2308 - val_loss: 1.9992 - val_accuracy: 0.0000e+00\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9323 - accuracy: 0.2308\n",
      "Epoch 69: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 32s 32s/step - loss: 1.9323 - accuracy: 0.2308 - val_loss: 1.9999 - val_accuracy: 0.0000e+00\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9321 - accuracy: 0.2308\n",
      "Epoch 70: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 32s 32s/step - loss: 1.9321 - accuracy: 0.2308 - val_loss: 2.0006 - val_accuracy: 0.0000e+00\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9320 - accuracy: 0.2308\n",
      "Epoch 71: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 32s 32s/step - loss: 1.9320 - accuracy: 0.2308 - val_loss: 2.0014 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9318 - accuracy: 0.2308\n",
      "Epoch 72: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 32s 32s/step - loss: 1.9318 - accuracy: 0.2308 - val_loss: 2.0021 - val_accuracy: 0.0000e+00\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9316 - accuracy: 0.2308\n",
      "Epoch 73: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 31s 31s/step - loss: 1.9316 - accuracy: 0.2308 - val_loss: 2.0028 - val_accuracy: 0.0000e+00\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9315 - accuracy: 0.2308\n",
      "Epoch 74: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 30s 30s/step - loss: 1.9315 - accuracy: 0.2308 - val_loss: 2.0035 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9313 - accuracy: 0.2308\n",
      "Epoch 75: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 30s 30s/step - loss: 1.9313 - accuracy: 0.2308 - val_loss: 2.0043 - val_accuracy: 0.0000e+00\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9312 - accuracy: 0.2308\n",
      "Epoch 76: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 31s 31s/step - loss: 1.9312 - accuracy: 0.2308 - val_loss: 2.0050 - val_accuracy: 0.0000e+00\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9310 - accuracy: 0.2308\n",
      "Epoch 77: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 32s 32s/step - loss: 1.9310 - accuracy: 0.2308 - val_loss: 2.0057 - val_accuracy: 0.0000e+00\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9308 - accuracy: 0.2308\n",
      "Epoch 78: val_loss did not improve from 1.85323\n",
      "1/1 [==============================] - 31s 31s/step - loss: 1.9308 - accuracy: 0.2308 - val_loss: 2.0064 - val_accuracy: 0.0000e+00\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/jaewone/developer/tensorflow/baby-cry-classification/model/wavenet.ipynb Cell 11\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jaewone/developer/tensorflow/baby-cry-classification/model/wavenet.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train, y_train, validation_data\u001b[39m=\u001b[39;49m(X_val, y_val), epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jaewone/developer/tensorflow/baby-cry-classification/model/wavenet.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m           batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m, optimizer\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39madam\u001b[39;49m\u001b[39m'\u001b[39;49m, save\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, save_dir\u001b[39m=\u001b[39;49mos\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(main_path, \u001b[39m'\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mhistory\u001b[39;49m\u001b[39m'\u001b[39;49m))\n",
      "\u001b[1;32m/Users/jaewone/developer/tensorflow/baby-cry-classification/model/wavenet.ipynb Cell 11\u001b[0m in \u001b[0;36mWaveNetClassifier.fit\u001b[0;34m(self, X, Y, validation_data, epochs, batch_size, optimizer, save, save_dir)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jaewone/developer/tensorflow/baby-cry-classification/model/wavenet.ipynb#X14sZmlsZQ%3D%3D?line=157'>158</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mcompile(optimizer, loss, metrics)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jaewone/developer/tensorflow/baby-cry-classification/model/wavenet.ipynb#X14sZmlsZQ%3D%3D?line=158'>159</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/jaewone/developer/tensorflow/baby-cry-classification/model/wavenet.ipynb#X14sZmlsZQ%3D%3D?line=159'>160</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhistory \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mfit(X, Y, shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, batch_size\u001b[39m=\u001b[39;49mbatch_size, epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jaewone/developer/tensorflow/baby-cry-classification/model/wavenet.ipynb#X14sZmlsZQ%3D%3D?line=160'>161</a>\u001b[0m                                   validation_data\u001b[39m=\u001b[39;49mvalidation_data, callbacks\u001b[39m=\u001b[39;49mcallbacks, initial_epoch\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstart_idx)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jaewone/developer/tensorflow/baby-cry-classification/model/wavenet.ipynb#X14sZmlsZQ%3D%3D?line=161'>162</a>\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jaewone/developer/tensorflow/baby-cry-classification/model/wavenet.ipynb#X14sZmlsZQ%3D%3D?line=162'>163</a>\u001b[0m     \u001b[39mif\u001b[39;00m save:\n",
      "File \u001b[0;32m~/ENTER/envs/tf25/lib/python3.9/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/ENTER/envs/tf25/lib/python3.9/site-packages/keras/engine/training.py:1384\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1377\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1378\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   1379\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   1380\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[1;32m   1381\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   1382\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m   1383\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1384\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1385\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1386\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/ENTER/envs/tf25/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/ENTER/envs/tf25/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/ENTER/envs/tf25/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/ENTER/envs/tf25/lib/python3.9/site-packages/tensorflow/python/eager/function.py:2956\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2953\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m   2954\u001b[0m   (graph_function,\n\u001b[1;32m   2955\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2956\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m   2957\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/ENTER/envs/tf25/lib/python3.9/site-packages/tensorflow/python/eager/function.py:1853\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1849\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1850\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1851\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1852\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1853\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1854\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1855\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m     args,\n\u001b[1;32m   1857\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1858\u001b[0m     executing_eagerly)\n\u001b[1;32m   1859\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/ENTER/envs/tf25/lib/python3.9/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/ENTER/envs/tf25/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=100,\n",
    "          batch_size=32, optimizer='adam', save=True, save_dir=os.path.join(main_path, 'model', 'history'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "for i in range(len(y_pred)):\n",
    "    real = label_encoder.classes_[np.argmax(y_test[i])]\n",
    "    pred_label = label_encoder.classes_[np.argmax(y_pred[i])]\n",
    "    print(f'Real: {real:>10} | Predict: {pred_label:>10} with {y_pred[i][np.argmax(y_pred[i])]*100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
